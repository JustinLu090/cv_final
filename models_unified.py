

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Dict, List, Tuple


class UnifiedTempoVLM(nn.Module):
    """
    Unified TempoVLM multi-task model with GRU Long-term Memory

    Tasks:
    - temporal: æ™‚åºä¸€è‡´æ€§ (ä½¿ç”¨ GRU é•·æœŸè¨˜æ†¶)
    - depth_order: æ·±åº¦æ’åº (A vs B èª°æ›´è¿‘) - ä½¿ç”¨ GT æ·±åº¦æ¨™ç±¤è¨“ç·´
    - depth_regression: ç›¸å°æ·±åº¦å€¼é æ¸¬ - ä½¿ç”¨ GT æ·±åº¦æ¨™ç±¤è¨“ç·´
    - motion: ç›¸æ©Ÿé‹å‹•é æ¸¬ (6DoF)
    
    GRU è¨˜æ†¶åŠŸèƒ½:
    - ç¶­è­·é•·æœŸéš±è—ç‹€æ…‹ï¼Œå³ä½¿é€£çºŒå¤šå¹€è¢«é®æ“‹ä¹Ÿèƒ½ä¿ç•™ä¹‹å‰çš„è³‡è¨Š
    - è‡ªå‹•å­¸ç¿’ä½•æ™‚æ›´æ–°/éºå¿˜è¨˜æ†¶
    
    Transformer Encoder:
    - ä½¿ç”¨å¤šå±¤ Transformer æ›¿ä»£ç°¡å–® Linearï¼Œæå‡ç‰¹å¾µè¡¨é”èƒ½åŠ›
    - Pre-LN æ¶æ§‹ç¢ºä¿è¨“ç·´ç©©å®šæ€§
    """
    
    def __init__(
        self,
        feat_dim: int = 1536,
        hidden_dim: int = 768,
        num_scene_classes: int = 20,
        dropout: float = 0.1,
        use_gru_memory: bool = True,  # æ˜¯å¦ä½¿ç”¨ GRU è¨˜æ†¶
        use_transformer_encoder: bool = True,  # æ˜¯å¦ä½¿ç”¨ Transformer Encoder
        num_encoder_layers: int = 2,  # Transformer å±¤æ•¸
        num_heads: int = 8,  # Attention head æ•¸é‡
    ):
        super().__init__()
        
        self.feat_dim = feat_dim
        self.hidden_dim = hidden_dim
        self.use_gru_memory = use_gru_memory
        self.use_transformer_encoder = use_transformer_encoder
        
        # ============================================================
        # shared encoder (Transformer æˆ– ç°¡å–® MLP)
        # ============================================================
        if use_transformer_encoder:
            # ä½¿ç”¨ Transformer Encoderï¼ˆæ›´å¼·çš„ç‰¹å¾µæå–ï¼‰
            self.input_proj = nn.Linear(feat_dim, hidden_dim)
            
            encoder_layer = nn.TransformerEncoderLayer(
                d_model=hidden_dim,
                nhead=num_heads,
                dim_feedforward=hidden_dim * 4,
                dropout=dropout,
                activation='gelu',
                batch_first=True,
                norm_first=True,  # Pre-LNï¼Œè¨“ç·´æ›´ç©©å®š
            )
            self.transformer_encoder = nn.TransformerEncoder(
                encoder_layer, 
                num_layers=num_encoder_layers
            )
            self.encoder_norm = nn.LayerNorm(hidden_dim)
            
            # ç‚ºäº†å‘å¾Œå…¼å®¹ï¼Œå‰µå»ºä¸€å€‹ wrapper
            def shared_encoder_forward(x):
                # x: [B, feat_dim]
                x = self.input_proj(x)  # [B, hidden_dim]
                x = x.unsqueeze(1)  # [B, 1, hidden_dim] - åŠ  sequence ç¶­åº¦
                x = self.transformer_encoder(x)  # [B, 1, hidden_dim]
                x = x.squeeze(1)  # [B, hidden_dim]
                return self.encoder_norm(x)
            
            # åŒ…è£æˆ moduleï¼ˆæ–¹ä¾¿åƒæ•¸ç®¡ç†ï¼‰
            class SharedEncoderWrapper(nn.Module):
                def __init__(self, forward_fn):
                    super().__init__()
                    self.forward_fn = forward_fn
                
                def forward(self, x):
                    return self.forward_fn(x)
            
            self.shared_encoder = SharedEncoderWrapper(shared_encoder_forward)
        else:
            # åŸå§‹ç°¡å–® MLPï¼ˆå‘å¾Œå…¼å®¹ï¼‰
            self.shared_encoder = nn.Sequential(
                nn.Linear(feat_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.GELU(),
                nn.Dropout(dropout),
            )
        
        # ============================================================
        # GRU Long-term Memory (NEW)
        # ============================================================
        if use_gru_memory:
            # GRU Cell: è¼¸å…¥ç•¶å‰è§€æ¸¬ï¼Œè¼¸å‡ºæ›´æ–°å¾Œçš„è¨˜æ†¶
            self.temporal_gru = nn.GRUCell(hidden_dim, hidden_dim)
            
            # è¨˜æ†¶å“è³ªè©•ä¼°å™¨ï¼šè©•ä¼°ç•¶å‰å¹€æ˜¯å¦å¯ä¿¡ï¼ˆç”¨æ–¼æ±ºå®šæ˜¯å¦æ›´æ–°è¨˜æ†¶ï¼‰
            self.memory_quality_gate = nn.Sequential(
                nn.Linear(hidden_dim * 2, hidden_dim),
                nn.GELU(),
                nn.Linear(hidden_dim, 1),
                nn.Sigmoid()
            )
            
            # è¨˜æ†¶èåˆé–€ï¼šæ±ºå®šè¼¸å‡ºæ™‚ä½¿ç”¨å¤šå°‘è¨˜æ†¶ vs ç•¶å‰è§€æ¸¬
            self.memory_output_gate = nn.Sequential(
                nn.Linear(hidden_dim * 2, hidden_dim),
                nn.Sigmoid()
            )
        
        # ============================================================
        # temporal consistency branch (ä¿ç•™åŸæœ‰çµæ§‹ä½œç‚ºå‚™ç”¨)
        # ============================================================
        self.temporal_fusion = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
        )
        
        self.temporal_gate = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.Sigmoid()
        )
        
        self.temporal_output = nn.Sequential(
            nn.Linear(hidden_dim, feat_dim),
        )
        
        # ============================================================
        # depth order branch
        # ============================================================
        self.depth_order_head = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, 2),  # [Aè¼ƒè¿‘, Bè¼ƒè¿‘]
        )
        
        # ============================================================
        # depth regression branch (predict absolute depth values for 3 regions)
        # ============================================================
        self.depth_regression_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.GELU(),
            nn.Linear(hidden_dim // 2, 3),  # è¼¸å‡º 3 å€‹å€åŸŸ: [left, center, right]
        )
        # æœ€å¤§æ·±åº¦ç¯„åœ (ç”¨æ–¼ sigmoid æ˜ å°„)
        self.max_depth = 10.0  # 10 meters
        
        # ============================================================
        # camera motion prediction branch
        # ============================================================
        self.motion_fusion = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
        )
        
        self.motion_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.GELU(),
            nn.Linear(hidden_dim // 2, 6),  # [tx, ty, tz, rx, ry, rz]
        )
        # é‹å‹•å°ºåº¦å› å­ (å¯å­¸ç¿’çš„åƒæ•¸) - åˆ†é›¢å¹³ç§»å’Œæ—‹è½‰çš„ scale
        # åˆå§‹åŒ–æ¥è¿‘ ScanNet çš„å…¸å‹é‹å‹•ç¯„åœï¼šå¹³ç§» ~0.01-0.1mï¼Œæ—‹è½‰ ~0.01-0.1rad
        self.motion_scale = nn.Parameter(torch.tensor([0.05, 0.05, 0.05, 0.02, 0.02, 0.02]))
        
        # ============================================================
        # è»Œè·¡ç´¯ç©èª¤å·®ä¿®æ­£æ¨¡çµ„ (NEW)
        # ============================================================
        # 1. Motion Uncertainty Head - é æ¸¬æ¯å¹€é‹å‹•çš„ä¸ç¢ºå®šæ€§
        self.motion_uncertainty_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 4),
            nn.GELU(),
            nn.Linear(hidden_dim // 4, 6),  # æ¯å€‹ç¶­åº¦çš„ log variance
        )
        
        # 2. Velocity Consistency - ç”¨æ–¼å¹³æ»‘è»Œè·¡
        self.velocity_smoothing = nn.Sequential(
            nn.Linear(hidden_dim + 6, hidden_dim // 2),  # ç•¶å‰ç‰¹å¾µ + å‰ä¸€å¹€é‹å‹•
            nn.GELU(),
            nn.Linear(hidden_dim // 2, 6),  # ä¿®æ­£é …
        )
        
        # 3. Global Scale Predictor - é æ¸¬å…¨å±€å°ºåº¦å› å­ï¼ˆè§£æ±º scale ä¸ä¸€è‡´å•é¡Œï¼‰
        self.global_scale_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 4),
            nn.GELU(),
            nn.Linear(hidden_dim // 4, 1),
            nn.Softplus(),  # ç¢ºä¿ scale > 0
        )
        
        # 4. Motion Quality Detector - æª¢æ¸¬å¿«é€Ÿé‹å‹•/æ¨¡ç³Šå¹€
        self.motion_quality_head = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim // 2),
            nn.GELU(),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid(),  # 0 = ä½å“è³ª, 1 = é«˜å“è³ª
        )
        
        # 5. Place Recognition - ç°¡åŒ–ç‰ˆ Loop Closureï¼ˆæª¢æ¸¬æ˜¯å¦å›åˆ°ç›¸ä¼¼ä½ç½®ï¼‰
        self.place_embedding = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
        )
       
        self.scene_classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, num_scene_classes),
        )
        
        self._init_weights()
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.LayerNorm):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
            elif isinstance(m, nn.GRUCell):
                # GRU ç‰¹æ®Šåˆå§‹åŒ–
                for name, param in m.named_parameters():
                    if 'weight' in name:
                        nn.init.orthogonal_(param)
                    elif 'bias' in name:
                        nn.init.zeros_(param)
    
    def init_hidden_state(self, batch_size: int, device: torch.device) -> torch.Tensor:
        """åˆå§‹åŒ– GRU éš±è—ç‹€æ…‹"""
        return torch.zeros(batch_size, self.hidden_dim, device=device)
    
    def load_pretrained_temporal(self, checkpoint_path: str, strict: bool = False):

        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        state_dict = checkpoint.get('model_state_dict', checkpoint)
        
        print(f"ğŸ“¦ åŸå§‹ checkpoint åŒ…å«çš„ keys:")
        for k, v in state_dict.items():
            print(f"   {k}: {v.shape}")
        
        compatible_keys = []
        incompatible_keys = []
        
        
        if 'gate.0.weight' in state_dict:
            old_weight = state_dict['gate.0.weight']  # [768, 3072]
            new_weight_shape = self.temporal_gate[0].weight.shape  # [768, 1536]
            
            if old_weight.shape[0] == new_weight_shape[0]:
            
                self.temporal_gate[0].weight.data = old_weight[:, :new_weight_shape[1]].clone()
                if 'gate.0.bias' in state_dict:
                    self.temporal_gate[0].bias.data = state_dict['gate.0.bias'].clone()
                compatible_keys.append('gate.0 (partial)')
        
        if 'refine.0.weight' in state_dict:
            old_shape = state_dict['refine.0.weight'].shape
            new_shape = self.temporal_output[0].weight.shape
            
            if old_shape == new_shape:
                self.temporal_output[0].weight.data = state_dict['refine.0.weight'].clone()
                self.temporal_output[0].bias.data = state_dict['refine.0.bias'].clone()
                compatible_keys.append('refine.0')
        
        print(f"\nâœ… é è¨“ç·´æ¬Šé‡è¼‰å…¥çµæœ:")
        print(f"   - éƒ¨åˆ†ç›¸å®¹: {compatible_keys}")
        print(f"   - çµæ§‹ä¸åŒï¼Œéœ€é‡æ–°è¨“ç·´: shared_encoder, temporal_fusion")
        print(f"   âš ï¸ ç”±æ–¼æ¶æ§‹å·®ç•°ï¼Œå»ºè­°é‡æ–°è¨“ç·´æˆ–ä½¿ç”¨ --no_pretrained")
        
        return compatible_keys
    
    def forward(
        self,
        curr_feat: torch.Tensor,
        prev_feat: Optional[torch.Tensor] = None,
        hidden_state: Optional[torch.Tensor] = None,  # GRU éš±è—ç‹€æ…‹ (NEW)
        region_a_feat: Optional[torch.Tensor] = None,
        region_b_feat: Optional[torch.Tensor] = None,
        tasks: List[str] = ['temporal'],
    ) -> Tuple[Dict[str, torch.Tensor], Optional[torch.Tensor]]:
        """
        Forward pass with optional GRU memory
        
        Args:
            curr_feat: ç•¶å‰å¹€ç‰¹å¾µ [B, feat_dim]
            prev_feat: å‰ä¸€å¹€ç‰¹å¾µ [B, feat_dim] (temporal/motion ç”¨)
            hidden_state: GRU éš±è—ç‹€æ…‹ [B, hidden_dim] (é•·æœŸè¨˜æ†¶)
            region_a_feat: å€åŸŸ A ç‰¹å¾µ [B, feat_dim] (depth_order ç”¨)
            region_b_feat: å€åŸŸ B ç‰¹å¾µ [B, feat_dim] (depth_order ç”¨)
            tasks: è¦åŸ·è¡Œçš„ä»»å‹™åˆ—è¡¨
        
        Returns:
            outputs: åŒ…å«å„ä»»å‹™è¼¸å‡ºçš„å­—å…¸
            next_hidden_state: æ›´æ–°å¾Œçš„ GRU éš±è—ç‹€æ…‹ (å¦‚æœä½¿ç”¨ GRU)
        """
        outputs = {}
        next_hidden_state = None
        
        # ç·¨ç¢¼ç•¶å‰å¹€
        curr_enc = self.shared_encoder(curr_feat)  # [B, hidden_dim]
        batch_size = curr_feat.shape[0]
        device = curr_feat.device
        
        # ============================================================
        # Task 1 : temporal consistency (with GRU Long-term Memory)
        # ============================================================
        if 'temporal' in tasks:
            if self.use_gru_memory:
                # ========== GRU é•·æœŸè¨˜æ†¶æ¨¡å¼ ==========
                
                # åˆå§‹åŒ–éš±è—ç‹€æ…‹ï¼ˆå¦‚æœæ˜¯ç¬¬ä¸€å¹€æˆ–æ–°å ´æ™¯ï¼‰
                if hidden_state is None:
                    hidden_state = self.init_hidden_state(batch_size, device)
                
                # 1. è©•ä¼°ç•¶å‰å¹€çš„å“è³ªï¼ˆæ˜¯å¦è¢«é®æ“‹ï¼‰
                #    æ¯”è¼ƒç•¶å‰è§€æ¸¬å’Œé•·æœŸè¨˜æ†¶çš„å·®ç•°
                combined_for_quality = torch.cat([curr_enc, hidden_state], dim=-1)
                quality_score = self.memory_quality_gate(combined_for_quality)  # [B, 1]
                
                # 2. GRU æ›´æ–°è¨˜æ†¶
                #    quality_score é«˜ = ç•¶å‰å¹€å¯ä¿¡ï¼Œå¤šæ›´æ–°è¨˜æ†¶
                #    quality_score ä½ = ç•¶å‰å¹€å¯èƒ½è¢«é®æ“‹ï¼Œå°‘æ›´æ–°è¨˜æ†¶
                gru_input = curr_enc * quality_score + hidden_state * (1 - quality_score)
                new_memory = self.temporal_gru(gru_input, hidden_state)
                
                # 3. æ±ºå®šè¼¸å‡ºæ™‚ä½¿ç”¨å¤šå°‘è¨˜æ†¶
                combined_for_output = torch.cat([curr_enc, new_memory], dim=-1)
                output_gate = self.memory_output_gate(combined_for_output)  # [B, hidden_dim]
                
                # 4. èåˆç•¶å‰è§€æ¸¬å’Œé•·æœŸè¨˜æ†¶
                fused_enc = output_gate * new_memory + (1 - output_gate) * curr_enc
                
                # 5. è¼¸å‡ºç²¾ç…‰å¾Œçš„ç‰¹å¾µ
                refined = self.temporal_output(fused_enc)
                
                # 6. æ®˜å·®é€£æ¥
                outputs['temporal'] = curr_feat + refined
                outputs['temporal_gate'] = output_gate.mean()
                outputs['memory_quality'] = quality_score.mean()  # ç”¨æ–¼ç›£æ§
                
                # æ›´æ–°éš±è—ç‹€æ…‹
                next_hidden_state = new_memory
                
            elif prev_feat is not None:
                # ========== åŸå§‹æ¨¡å¼ï¼ˆç„¡ GRUï¼‰==========
                prev_enc = self.shared_encoder(prev_feat)
                
                # fusion
                combined = torch.cat([curr_enc, prev_enc], dim=-1)
                fused = self.temporal_fusion(combined)
                
                # gate
                gate = self.temporal_gate(combined)
                gated = fused * gate + curr_enc * (1 - gate)

                # output refined features
                refined = self.temporal_output(gated)
                
                # residual connection
                outputs['temporal'] = curr_feat + refined
                outputs['temporal_gate'] = gate.mean() 
        
        # ============================================================
        # Task 2 : depth order
        # ============================================================
        if 'depth_order' in tasks:
            if region_a_feat is not None and region_b_feat is not None:
                region_a_enc = self.shared_encoder(region_a_feat)
                region_b_enc = self.shared_encoder(region_b_feat)
                combined = torch.cat([region_a_enc, region_b_enc], dim=-1)
                outputs['depth_order'] = self.depth_order_head(combined)  # [B, 2]
            else:
                # ä½¿ç”¨å…¨åœ–ç‰¹å¾µçš„ä¸åŒå€åŸŸï¼ˆç°¡åŒ–ç‰ˆï¼‰
                outputs['depth_order'] = None
        
        # ============================================================
        # Task 3 : depth regression (è¼¸å‡º 3 å€‹å€åŸŸçš„çµ•å°æ·±åº¦)
        # ============================================================
        if 'depth_regression' in tasks:
            raw_depth = self.depth_regression_head(curr_enc)  # [B, 3]
            # ä½¿ç”¨ softplus ç¢ºä¿è¼¸å‡ºç‚ºæ­£æ•¸ï¼Œä¸¦é™åˆ¶åœ¨åˆç†ç¯„åœå…§
            # softplus(x) = log(1 + exp(x))ï¼Œå¹³æ»‘çš„ ReLU
            depth = F.softplus(raw_depth) * (self.max_depth / 5.0)  # scale to ~0-10m range
            # æˆ–è€…ç”¨ sigmoid: depth = torch.sigmoid(raw_depth) * self.max_depth
            outputs['depth_regression'] = depth  # [B, 3] = [left, center, right]
        
        # ============================================================
        # Task 4 : camera motion prediction (with quality & scale correction)
        # ============================================================
        if 'motion' in tasks and prev_feat is not None:
            prev_enc = self.shared_encoder(prev_feat)
            combined = torch.cat([curr_enc, prev_enc], dim=-1)
            fused = self.motion_fusion(combined)
            raw_motion = self.motion_head(fused)  # [B, 6]
            
            # 1. åŸºç¤é‹å‹•é æ¸¬ + scale åƒæ•¸
            motion = raw_motion * self.motion_scale.unsqueeze(0)  # [B, 6]
            
            # 2. é æ¸¬é‹å‹•ä¸ç¢ºå®šæ€§ (ç”¨æ–¼åŠ æ¬Š loss)
            motion_log_var = self.motion_uncertainty_head(fused)  # [B, 6]
            motion_uncertainty = torch.exp(motion_log_var)  # [B, 6]
            
            # 3. é æ¸¬å…¨å±€ scale factor (ç”¨æ–¼æ ¡æ­£ç´¯ç©èª¤å·®)
            global_scale = self.global_scale_head(curr_enc)  # [B, 1]
            # å°‡ global_scale é™åˆ¶åœ¨åˆç†ç¯„åœ [0.5, 2.0]
            global_scale = 0.5 + 1.5 * torch.sigmoid(global_scale - 1)
            
            # 4. æª¢æ¸¬é‹å‹•å“è³ªï¼ˆå¿«é€Ÿé‹å‹•/æ¨¡ç³Šæª¢æ¸¬ï¼‰
            motion_quality = self.motion_quality_head(combined)  # [B, 1]
            
            # 5. Place Recognition embedding (ç”¨æ–¼ Loop Closure)
            place_emb = self.place_embedding(curr_enc)  # [B, hidden_dim//2]
            
            outputs['motion'] = motion
            outputs['motion_raw'] = raw_motion  # åŸå§‹é æ¸¬ï¼ˆç”¨æ–¼åˆ†æï¼‰
            outputs['motion_uncertainty'] = motion_uncertainty
            outputs['motion_log_var'] = motion_log_var
            outputs['motion_global_scale'] = global_scale
            outputs['motion_quality'] = motion_quality
            outputs['place_embedding'] = place_emb
        
        if 'scene_class' in tasks:
            outputs['scene_class'] = self.scene_classifier(curr_enc)  # [B, num_classes]
        
        # è¿”å› outputs å’Œ next_hidden_stateï¼ˆå¦‚æœä½¿ç”¨ GRU è¨˜æ†¶ï¼‰
        if self.use_gru_memory and 'temporal' in tasks:
            return outputs, next_hidden_state
        else:
            return outputs, None
    
    def forward_temporal(
        self, 
        curr_feat: torch.Tensor, 
        prev_feat: torch.Tensor = None,
        hidden_state: torch.Tensor = None
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """ä¾¿åˆ©æ–¹æ³•ï¼šåªåŸ·è¡Œ temporal ä»»å‹™"""
        outputs, next_hidden = self.forward(
            curr_feat, prev_feat, 
            hidden_state=hidden_state,
            tasks=['temporal']
        )
        return outputs['temporal'], next_hidden
    
    def forward_depth_order(
        self, 
        region_a_feat: torch.Tensor, 
        region_b_feat: torch.Tensor
    ) -> torch.Tensor:
        outputs, _ = self.forward(
            region_a_feat, 
            region_a_feat=region_a_feat,
            region_b_feat=region_b_feat,
            tasks=['depth_order']
        )
        return outputs['depth_order']
    
    def forward_motion(
        self, 
        curr_feat: torch.Tensor, 
        prev_feat: torch.Tensor
    ) -> torch.Tensor:
        outputs, _ = self.forward(curr_feat, prev_feat, tasks=['motion'])
        return outputs['motion']


class UnifiedLoss(nn.Module):
    """
    Unified multi-task loss for UnifiedTempoVLM
    
    é‡æ–°è¨­è¨ˆçš„ Loss å¹³è¡¡ç­–ç•¥ï¼š
    1. ä½¿ç”¨å›ºå®šçš„ loss å°ºåº¦æ­¸ä¸€åŒ–ï¼Œç¢ºä¿æ¯å€‹ä»»å‹™è²¢ç»å‡è¡¡
    2. å° InfoNCE loss é€²è¡Œç‰¹æ®Šè™•ç†ï¼ˆå€¼å¾ˆå¤§ï¼‰
    3. æ‰‹å‹•è¨­å®šä»»å‹™å„ªå…ˆç´šæ¬Šé‡
    """
    def __init__(
        self,
        num_tasks: int = 5,
        use_uncertainty_weighting: bool = False,  # é è¨­é—œé–‰è‡ªå‹•æ¬Šé‡
        # æ‰‹å‹•æ¬Šé‡è¨­å®šï¼ˆç¶“éèª¿æ ¡ï¼‰
        task_weights: Dict[str, float] = None,
    ):
        super().__init__()
        
        self.use_uncertainty_weighting = use_uncertainty_weighting
        
        # ============================================================
        # æ‰‹å‹•è¨­å®šçš„ä»»å‹™æ¬Šé‡ï¼ˆç¶“éåˆ†æèª¿æ ¡ï¼‰
        # ============================================================
        if task_weights is None:
            self.task_weights = {
                'temporal': 0.1,          # InfoNCE loss å¤ªå¤§ï¼Œé™ä½æ¬Šé‡
                'depth_order': 1.0,       # åˆ†é¡ä»»å‹™ï¼Œä¿æŒæ¨™æº–æ¬Šé‡
                'depth_regression': 3.0,  # ğŸ”¥ æé«˜æ·±åº¦å›æ­¸æ¬Šé‡
                'motion': 2.0,            # ğŸ”¥ æé«˜é‹å‹•é æ¸¬æ¬Šé‡
                'scene_class': 0.5,       # è¼”åŠ©ä»»å‹™ï¼Œé™ä½æ¬Šé‡
                'occlusion_recon': 1.5,   # é®æ“‹é‡å»º
                'memory_quality_reg': 0.5, # è¨˜æ†¶å“è³ªæ­£å‰‡åŒ–
            }
        else:
            self.task_weights = task_weights
        
        # å¯å­¸ç¿’çš„ log variance åƒæ•¸ (å‚™ç”¨)
        if use_uncertainty_weighting:
            # ç”¨æ›´å¥½çš„åˆå§‹åŒ–ï¼šdepth å’Œ motion çš„åˆå§‹æ¬Šé‡æ›´é«˜
            init_log_vars = torch.tensor([2.0, 0.0, -1.0, -0.5, 0.5])  # å°æ‡‰æ¬Šé‡: [0.14, 1.0, 2.7, 1.6, 0.6]
            self.log_vars = nn.Parameter(init_log_vars)
        
        self.ce_loss = nn.CrossEntropyLoss()
        self.mse_loss = nn.MSELoss()
        
        # ç”¨æ–¼ loss å°ºåº¦è¿½è¹¤çš„ EMA
        self.register_buffer('loss_ema', torch.ones(num_tasks))
        self.ema_decay = 0.99
    
    def _get_weight(self, task_name: str, task_idx: int = None) -> float:
        """ç²å–ä»»å‹™æ¬Šé‡"""
        if self.use_uncertainty_weighting and task_idx is not None:
            log_var = torch.clamp(self.log_vars[task_idx], min=-4, max=4)
            return torch.exp(-log_var)
        else:
            return self.task_weights.get(task_name, 1.0)
    
    def scale_invariant_depth_loss(self, pred, target):
        """
        æ”¹é€²çš„æ·±åº¦ Lossï¼š
        1. Scale-Invariant Loss
        2. L1 Loss
        3. æ¢¯åº¦ Lossï¼ˆé¼“å‹µå¹³æ»‘ï¼‰
        """
        valid_mask = target > 0.1
        
        if valid_mask.sum() == 0:
            return torch.tensor(0.0, device=pred.device, requires_grad=True)
        
        pred_valid = pred[valid_mask].clamp(min=1e-6)
        target_valid = target[valid_mask].clamp(min=1e-6)
        
        # 1. L1 Lossï¼ˆä¸»è¦ï¼‰
        l1_loss = F.l1_loss(pred_valid, target_valid)
        
        # 2. Scale-Invariant Lossï¼ˆè¼”åŠ©ï¼‰
        log_diff = torch.log(pred_valid) - torch.log(target_valid)
        n = log_diff.numel()
        if n > 0:
            si_loss = torch.sum(log_diff ** 2) / n - 0.5 * (torch.sum(log_diff) ** 2) / (n ** 2)
        else:
            si_loss = torch.tensor(0.0, device=pred.device)
        
        # 3. ç›¸å°èª¤å·® Lossï¼ˆé¼“å‹µæ¯”ä¾‹æ­£ç¢ºï¼‰
        rel_loss = (torch.abs(pred_valid - target_valid) / (target_valid + 1e-6)).mean()
        
        return l1_loss + 0.5 * si_loss + 0.3 * rel_loss
    
    def motion_loss(self, pred, target, log_var=None):
        """
        ç°¡åŒ–çš„é‹å‹• Lossï¼š
        - å¹³ç§»ç”¨ Smooth L1
        - æ—‹è½‰ç”¨ MSE
        """
        pred_trans = pred[:, :3]
        pred_rot = pred[:, 3:]
        target_trans = target[:, :3]
        target_rot = target[:, 3:]
        
        # ç°¡å–®ç›´æ¥çš„ lossï¼Œä¸ç”¨ä¸ç¢ºå®šæ€§
        trans_loss = F.smooth_l1_loss(pred_trans, target_trans)
        rot_loss = F.mse_loss(pred_rot, target_rot)
        
        return trans_loss + rot_loss
    
    def temporal_contrastive_loss(self, refined_curr, prev_feat):
        """
        æ”¹é€²çš„æ™‚åºå°æ¯” Lossï¼š
        1. ä½¿ç”¨æ›´é«˜çš„æº«åº¦åƒæ•¸ï¼ˆé¿å… loss éå¤§ï¼‰
        2. åŠ å…¥æ­£æ¨£æœ¬ç›¸ä¼¼åº¦ç´„æŸ
        """
        batch_size = refined_curr.shape[0]
        
        if batch_size <= 1:
            return 1 - F.cosine_similarity(refined_curr.float(), prev_feat.float(), dim=-1).mean(), {}, {}
        
        # æ­£å‰‡åŒ–ç‰¹å¾µ
        refined_norm = F.normalize(refined_curr, p=2, dim=-1)
        prev_norm = F.normalize(prev_feat, p=2, dim=-1)
        
        # è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
        sim_matrix = refined_norm @ prev_norm.T
        
        # ğŸ”¥ ä½¿ç”¨æ›´é«˜çš„æº«åº¦ï¼Œé¿å… loss éå¤§
        tau = 0.1  # å¾ 0.02 æé«˜åˆ° 0.1
        
        # InfoNCE Loss
        exp_sim = torch.exp(sim_matrix / tau)
        pos_exp = torch.diag(exp_sim)
        
        mask = torch.eye(batch_size, device=sim_matrix.device).bool()
        neg_exp_sum = exp_sim.masked_fill(mask, 0).sum(dim=1)
        
        infonce_loss = -torch.log(pos_exp / (pos_exp + neg_exp_sum + 1e-8)).mean()
        
        # ğŸ”¥ åŠ å…¥æ­£æ¨£æœ¬ç›¸ä¼¼åº¦ç´„æŸï¼šé¼“å‹µæ­£æ¨£æœ¬ç›¸ä¼¼åº¦ > 0.8
        pos_sim = torch.diag(sim_matrix)
        pos_sim_loss = F.relu(0.8 - pos_sim).mean()  # å¦‚æœç›¸ä¼¼åº¦ < 0.8ï¼Œæœ‰æ‡²ç½°
        
        # çµ„åˆ lossï¼ˆæ§åˆ¶ InfoNCE çš„å½±éŸ¿ï¼‰
        # InfoNCE é€šå¸¸åœ¨ 2-5 ä¹‹é–“ï¼Œæˆ‘å€‘å¸Œæœ›ç¸½ loss åœ¨ 0.5-2 ä¹‹é–“
        total_loss = 0.3 * infonce_loss + 0.7 * pos_sim_loss
        
        # è¨ºæ–·ä¿¡æ¯
        with torch.no_grad():
            neg_sim = sim_matrix.masked_fill(mask, 0).sum() / (batch_size * (batch_size - 1))
        
        return total_loss, {'pos_sim': pos_sim.mean().item(), 'neg_sim': neg_sim.item()}, {'infonce': infonce_loss.item()}
    
    def forward(
        self,
        outputs: Dict[str, torch.Tensor],
        targets: Dict[str, torch.Tensor],
        prev_feat: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, Dict[str, float]]:
        """
        è¨ˆç®—å¤šä»»å‹™ Loss
        
        æ”¹é€²ï¼š
        1. æ¯å€‹ loss éƒ½æœ‰æ˜ç¢ºçš„å°ºåº¦ç¯„åœ
        2. æ‰‹å‹•è¨­å®šçš„æ¬Šé‡ç¢ºä¿å¹³è¡¡
        3. è©³ç´°çš„è¨ºæ–·è¼¸å‡º
        """
        total_loss = torch.tensor(0.0, device=next(iter(outputs.values())).device)
        loss_dict = {}
        
        # ============================================================
        # Task 0: Temporal Consistency (å°æ¯”å­¸ç¿’)
        # ç›®æ¨™ç¯„åœ: 0.3 - 1.0
        # ============================================================
        if 'temporal' in outputs and prev_feat is not None:
            temporal_loss, diag_info, raw_losses = self.temporal_contrastive_loss(
                outputs['temporal'], prev_feat
            )
            
            weight = self._get_weight('temporal', 0)
            total_loss = total_loss + weight * temporal_loss
            
            loss_dict['temporal'] = temporal_loss.item()
            loss_dict['temporal_weight'] = weight if isinstance(weight, float) else weight.item()
            if diag_info:
                loss_dict['temporal_pos_sim'] = diag_info['pos_sim']
                loss_dict['temporal_neg_sim'] = diag_info['neg_sim']
            if raw_losses:
                loss_dict['temporal_infonce_raw'] = raw_losses['infonce']
        
        # ============================================================
        # Task 1: Depth Order (åˆ†é¡)
        # ç›®æ¨™ç¯„åœ: 0.3 - 1.0
        # ============================================================
        if 'depth_order' in outputs and outputs['depth_order'] is not None:
            if 'depth_order' in targets:
                depth_order_loss = self.ce_loss(outputs['depth_order'], targets['depth_order'])
                
                weight = self._get_weight('depth_order', 1)
                total_loss = total_loss + weight * depth_order_loss
                
                loss_dict['depth_order'] = depth_order_loss.item()
                loss_dict['depth_order_weight'] = weight if isinstance(weight, float) else weight.item()
        
        # ============================================================
        # Task 2: Depth Regression (å›æ­¸) ğŸ”¥ é‡è¦ä»»å‹™
        # ç›®æ¨™ç¯„åœ: 0.1 - 0.5
        # ============================================================
        if 'depth_regression' in outputs and outputs['depth_regression'] is not None:
            if 'depth_regression' in targets:
                pred_depth = outputs['depth_regression']
                target_depth = targets['depth_regression']
                
                if target_depth.dim() == 1:
                    target_depth = target_depth.unsqueeze(-1).expand(-1, 3)
                elif target_depth.shape[-1] == 1:
                    target_depth = target_depth.expand(-1, 3)
                
                depth_reg_loss = self.scale_invariant_depth_loss(pred_depth, target_depth)
                
                weight = self._get_weight('depth_regression', 2)
                total_loss = total_loss + weight * depth_reg_loss
                
                loss_dict['depth_regression'] = depth_reg_loss.item()
                loss_dict['depth_regression_weight'] = weight if isinstance(weight, float) else weight.item()
                
                # é¡å¤–è¨˜éŒ„åŸå§‹èª¤å·®
                with torch.no_grad():
                    raw_error = F.l1_loss(pred_depth, target_depth)
                    loss_dict['depth_l1_error'] = raw_error.item()
        
        # ============================================================
        # Task 3: Motion Prediction (å›æ­¸) ğŸ”¥ é‡è¦ä»»å‹™
        # ç›®æ¨™ç¯„åœ: 0.05 - 0.3
        # ============================================================
        if 'motion' in outputs and 'motion' in targets:
            motion_loss = self.motion_loss(outputs['motion'], targets['motion'])
            
            weight = self._get_weight('motion', 3)
            total_loss = total_loss + weight * motion_loss
            
            loss_dict['motion'] = motion_loss.item()
            loss_dict['motion_weight'] = weight if isinstance(weight, float) else weight.item()
        
        # ============================================================
        # Task 4: Scene Classification (è¼”åŠ©ä»»å‹™)
        # ============================================================
        if 'scene_class' in outputs and 'scene_class' in targets:
            scene_loss = self.ce_loss(outputs['scene_class'], targets['scene_class'])
            
            weight = self._get_weight('scene_class', 4)
            total_loss = total_loss + weight * scene_loss
            
            loss_dict['scene_class'] = scene_loss.item()
            loss_dict['scene_class_weight'] = weight if isinstance(weight, float) else weight.item()
        
        # ============================================================
        # é®æ“‹é‡å»º Loss (å¦‚æœæœ‰)
        # ============================================================
        if 'occlusion_reconstruction' in outputs and 'clean_features' in targets:
            recon_loss = F.mse_loss(
                outputs['occlusion_reconstruction'],
                targets['clean_features']
            )
            weight = self._get_weight('occlusion_recon')
            total_loss = total_loss + weight * recon_loss
            loss_dict['occlusion_recon'] = recon_loss.item()
        
        # ============================================================
        # Memory Quality æ­£å‰‡åŒ– (ğŸ”¥ å¼·åŒ–ç‰ˆ)
        # é˜²æ­¢ memory_quality è¶¨å‘ 0 æˆ– 1
        # ============================================================
        if 'memory_quality' in outputs:
            mq = outputs['memory_quality']
            
            # é›™å‘æ‡²ç½°ï¼šé¼“å‹µ mq åœ¨ 0.3-0.7 ä¹‹é–“
            # ä½æ–¼ 0.3 çš„æ‡²ç½°æ›´å¼·ï¼ˆé˜²æ­¢ GRU ä¸å·¥ä½œï¼‰
            mq_reg_low = torch.clamp(0.35 - mq, min=0) ** 2 * 2.0  # ä½æ–¼ 0.35 å¼·æ‡²ç½°
            mq_reg_high = torch.clamp(mq - 0.65, min=0) ** 2       # é«˜æ–¼ 0.65 è¼•æ‡²ç½°
            
            # é¡å¤–çš„ä¸­å¿ƒåŒ– lossï¼šé¼“å‹µæ¥è¿‘ 0.5
            mq_center = (mq - 0.5) ** 2 * 0.1
            
            mq_reg = (mq_reg_low + mq_reg_high + mq_center).mean()
            
            weight = self._get_weight('memory_quality_reg')
            total_loss = total_loss + weight * mq_reg
            
            loss_dict['memory_quality'] = mq.mean().item()
            loss_dict['memory_quality_reg'] = mq_reg.item()
        
        # ============================================================
        # ç¸½ Loss è¨ºæ–·
        # ============================================================
        loss_dict['total_loss'] = total_loss.item()
        
        return total_loss, loss_dict
    
    def get_task_weights(self) -> Dict[str, float]:
        """å–å¾—ç•¶å‰å„ä»»å‹™çš„æ¬Šé‡"""
        if self.use_uncertainty_weighting:
            task_names = ['temporal', 'depth_order', 'depth_regression', 'motion', 'scene_class']
            weights = torch.exp(-self.log_vars).detach().cpu().numpy()
            return {name: float(w) for name, w in zip(task_names, weights)}
        else:
            return self.task_weights.copy()


# ============================================================
# tools
# ============================================================

def create_unified_model(
    feat_dim: int = 1536,
    pretrained_temporal_path: Optional[str] = None,
) -> UnifiedTempoVLM:

    model = UnifiedTempoVLM(feat_dim=feat_dim)
    
    if pretrained_temporal_path:
        model.load_pretrained_temporal(pretrained_temporal_path)
    
    return model


def get_model_info(model: UnifiedTempoVLM) -> Dict:
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    # å„åˆ†æ”¯åƒæ•¸é‡
    branch_params = {}
    
    # Shared Encoder (åŒ…å« Transformer æˆ– MLP)
    if model.use_transformer_encoder:
        encoder_params = sum(p.numel() for p in model.input_proj.parameters())
        encoder_params += sum(p.numel() for p in model.transformer_encoder.parameters())
        encoder_params += sum(p.numel() for p in model.encoder_norm.parameters())
        branch_params['shared_encoder (Transformer)'] = encoder_params
    else:
        branch_params['shared_encoder (MLP)'] = sum(p.numel() for p in model.shared_encoder.parameters())
    
    branch_params['temporal'] = sum(p.numel() for n, p in model.named_parameters() if 'temporal' in n and 'gru' not in n.lower() and 'transformer' not in n.lower())
    branch_params['depth_order'] = sum(p.numel() for p in model.depth_order_head.parameters())
    branch_params['depth_regression'] = sum(p.numel() for p in model.depth_regression_head.parameters())
    branch_params['motion'] = sum(p.numel() for n, p in model.named_parameters() if 'motion' in n)
    branch_params['scene_classifier'] = sum(p.numel() for p in model.scene_classifier.parameters())
    
    # GRU è¨˜æ†¶ç›¸é—œåƒæ•¸
    if model.use_gru_memory:
        gru_params = sum(p.numel() for p in model.temporal_gru.parameters())
        memory_gate_params = sum(p.numel() for p in model.memory_quality_gate.parameters())
        memory_gate_params += sum(p.numel() for p in model.memory_output_gate.parameters())
        branch_params['gru_memory'] = gru_params + memory_gate_params
    
    return {
        'total_params': total_params,
        'trainable_params': trainable_params,
        'branch_params': branch_params,
    }


# ============================================================
# Testing
# ============================================================

if __name__ == "__main__":
    print("Testing UnifiedTempoVLM...")

    # æ¸¬è©¦ Transformer Encoder æ¨¡å¼
    print("\n========== æ¸¬è©¦ Transformer Encoder æ¨¡å¼ ==========")
    model = UnifiedTempoVLM(
        feat_dim=1536, 
        hidden_dim=768, 
        use_gru_memory=False,
        use_transformer_encoder=True,
        num_encoder_layers=2,
        num_heads=8
    )
    model.eval()
    
    batch_size = 2
    curr_feat = torch.randn(batch_size, 1536)
    prev_feat = torch.randn(batch_size, 1536)
    region_a = torch.randn(batch_size, 1536)
    region_b = torch.randn(batch_size, 1536)
    
    print("\nTesting multi-task forward propagation...")
    outputs, hidden = model(
        curr_feat=curr_feat,
        prev_feat=prev_feat,
        region_a_feat=region_a,
        region_b_feat=region_b,
        tasks=['temporal', 'depth_order', 'depth_regression', 'motion', 'scene_class']
    )
    
    print(f"  temporal output: {outputs['temporal'].shape}")
    print(f"  depth_order output: {outputs['depth_order'].shape}")
    print(f"  depth_regression output: {outputs['depth_regression'].shape}")  # æ‡‰è©²æ˜¯ [B, 3]
    print(f"  depth_regression values: {outputs['depth_regression']}")  # æª¢æŸ¥æ˜¯å¦ç‚ºæ­£æ•¸
    print(f"  motion output: {outputs['motion'].shape}")
    print(f"  motion values: {outputs['motion']}")  # æª¢æŸ¥é‹å‹•å€¼
    print(f"  scene_class output: {outputs['scene_class'].shape}")
    print(f"  next_hidden_state: {hidden}")  # æ‡‰è©²æ˜¯ Noneï¼ˆç„¡ GRU æ¨¡å¼ï¼‰

    # æ¸¬è©¦ GRU + Transformer çµ„åˆæ¨¡å¼
    print("\n========== æ¸¬è©¦ GRU + Transformer çµ„åˆæ¨¡å¼ ==========")
    model_gru = UnifiedTempoVLM(
        feat_dim=1536, 
        hidden_dim=768, 
        use_gru_memory=True,
        use_transformer_encoder=True,
        num_encoder_layers=2,
        num_heads=8
    )
    model_gru.eval()
    
    print("\næ¨¡æ“¬é€£çºŒå¹€è™•ç†...")
    hidden_state = None
    for frame_idx in range(5):
        curr_feat = torch.randn(batch_size, 1536)
        outputs, hidden_state = model_gru(
            curr_feat=curr_feat,
            hidden_state=hidden_state,
            tasks=['temporal']
        )
        print(f"  Frame {frame_idx}: temporal_gate={outputs.get('temporal_gate', 'N/A'):.3f}, "
              f"memory_quality={outputs.get('memory_quality', 'N/A'):.3f}, "
              f"hidden_state shape={hidden_state.shape if hidden_state is not None else 'None'}")

    # æ¸¬è©¦èˆŠç‰ˆ MLP æ¨¡å¼ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
    print("\n========== æ¸¬è©¦èˆŠç‰ˆ MLP æ¨¡å¼ï¼ˆå‘å¾Œå…¼å®¹ï¼‰==========")
    model_mlp = UnifiedTempoVLM(
        feat_dim=1536, 
        hidden_dim=768, 
        use_gru_memory=False,
        use_transformer_encoder=False  # ä½¿ç”¨èˆŠç‰ˆ MLP
    )
    model_mlp.eval()
    
    outputs_mlp, _ = model_mlp(
        curr_feat=curr_feat,
        prev_feat=prev_feat,
        region_a_feat=region_a,
        region_b_feat=region_b,
        tasks=['temporal', 'depth_order', 'depth_regression', 'motion']
    )
    print(f"  MLP mode - temporal output: {outputs_mlp['temporal'].shape}")
    print(f"  MLP mode - depth_regression output: {outputs_mlp['depth_regression'].shape}")

    print("\nTesting loss calculation with automatic weighting...")
    loss_fn = UnifiedLoss(use_uncertainty_weighting=True)
    
    # æ¨¡æ“¬ GT æ·±åº¦æ¨™ç±¤ï¼ˆä¾†è‡ª ScanNet æ•¸æ“šé›†ï¼‰
    targets = {
        'depth_order': torch.randint(0, 2, (batch_size,)),
        'depth_regression': torch.rand(batch_size, 3) * 5 + 0.5,  # 0.5~5.5m çš„æ·±åº¦ï¼ˆGTï¼‰
        'motion': torch.randn(batch_size, 6) * 0.1,  # å°çš„é‹å‹•å€¼
        'scene_class': torch.randint(0, 20, (batch_size,)),
    }
    
    # ä½¿ç”¨ Transformer æ¨¡å‹çš„ outputs
    outputs, _ = model(
        curr_feat=torch.randn(batch_size, 1536),
        prev_feat=torch.randn(batch_size, 1536),
        region_a_feat=region_a,
        region_b_feat=region_b,
        tasks=['temporal', 'depth_order', 'depth_regression', 'motion', 'scene_class']
    )
    
    total_loss, loss_dict = loss_fn(outputs, targets, prev_feat)
    print(f"  Total loss: {total_loss.item():.4f}")
    print(f"  Individual losses:")
    for k, v in loss_dict.items():
        if not k.endswith('_weight'):
            print(f"    {k}: {v:.4f}")
    
    print(f"\n  Auto-learned task weights (åˆå§‹æ‡‰è©²éƒ½æ¥è¿‘ 1.0):")
    weights = loss_fn.get_task_weights()
    for task, weight in weights.items():
        print(f"    {task}: {weight:.4f}")
    
    print(f"\n  Log variance parameters:")
    for i, name in enumerate(['temporal', 'depth_order', 'depth_regression', 'motion', 'scene_class']):
        print(f"    {name}: log_var = {loss_fn.log_vars[i].item():.4f}")

    print("\n========== Model Information ==========")
    
    print("\nğŸ“Š Transformer Encoder æ¨¡å¼:")
    info = get_model_info(model)
    print(f"  Total Parameters: {info['total_params']:,}")
    print(f"  Branch Parameters:")
    for branch, params in info['branch_params'].items():
        print(f"    {branch}: {params:,}")
    
    print("\nğŸ§  GRU + Transformer æ¨¡å¼:")
    info_gru = get_model_info(model_gru)
    print(f"  Total Parameters: {info_gru['total_params']:,}")
    print(f"  Branch Parameters:")
    for branch, params in info_gru['branch_params'].items():
        print(f"    {branch}: {params:,}")
    
    print("\nğŸ“¦ èˆŠç‰ˆ MLP æ¨¡å¼:")
    info_mlp = get_model_info(model_mlp)
    print(f"  Total Parameters: {info_mlp['total_params']:,}")
    print(f"  Branch Parameters:")
    for branch, params in info_mlp['branch_params'].items():
        print(f"    {branch}: {params:,}")
    
    # æ¯”è¼ƒåƒæ•¸å¢é‡
    param_increase = info['total_params'] - info_mlp['total_params']
    print(f"\nğŸ“ˆ Transformer ç›¸æ¯” MLP å¢åŠ åƒæ•¸: {param_increase:,} ({param_increase/info_mlp['total_params']*100:.1f}%)")
    
    # æ¸¬è©¦ Loss å‡½æ•¸çš„åƒæ•¸é‡
    loss_params = sum(p.numel() for p in loss_fn.parameters())
    print(f"\n  Loss function learnable params: {loss_params}")
    
    print("\n" + "="*60)
    print("ğŸ’¡ é‡è¦èªªæ˜:")
    print("  1. æ·±åº¦æ¨™ç±¤ï¼ˆdepth_regressionï¼‰ä¾†è‡ª ScanNet çš„ GT æ·±åº¦åœ–")
    print("  2. æ¨¡å‹å­¸ç¿’å¾ RGB ç‰¹å¾µé æ¸¬æ·±åº¦å€¼")
    print("  3. Transformer Encoder æä¾›æ›´å¼·çš„ç‰¹å¾µè¡¨é”èƒ½åŠ›")
    print("  4. å¯ä»¥ç”¨ use_transformer_encoder=False åˆ‡æ›å›èˆŠç‰ˆ MLP")
    print("="*60)

    print("\nâœ… Testing completed!")
