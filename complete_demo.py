#!/usr/bin/env python3
"""
TempoVLM å®Œæ•´è¦–è¦ºåŒ–å±•ç¤ºè…³æœ¬
===========================

æ•´åˆæ‰€æœ‰è¦–è¦ºåŒ–åŠŸèƒ½ï¼š
1. æ™‚åºç©©å®šæ€§ - Split Screen é®æ“‹æ¸¬è©¦å½±ç‰‡
2. æ·±åº¦æ„ŸçŸ¥ - Depth Ordering å„€è¡¨æ¿
3. é‹å‹•æ„ŸçŸ¥ - Real-time Trajectory Plot
4. é®æ“‹æ¸¬è©¦ - Occlusion Detection & Memory Injection (NEW)

è¼¸å‡ºï¼š
- å°æ¯”å½±ç‰‡
- å„€è¡¨æ¿æˆªåœ–
- è»Œè·¡å‹•ç•«
- é®æ“‹æ¸¬è©¦å ±å‘Šèˆ‡çµ±è¨ˆ
"""

import os
import sys
import json
import torch
import torch.nn.functional as F
import numpy as np
from pathlib import Path
from PIL import Image, ImageDraw, ImageFont
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')
from matplotlib.patches import Circle, Wedge
from matplotlib.collections import PatchCollection
import cv2
import argparse
from collections import deque
from datetime import datetime

# Qwen2-VL
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info

from utils.memory_utils import AdaptiveMemoryBuffer

# YOLO ç‰©ä»¶é®æ“‹ï¼ˆå¯é¸ï¼‰
try:
    from utils.yolo_occlusion import YOLOOccluder
    YOLO_AVAILABLE = True
except ImportError:
    YOLO_AVAILABLE = False
    YOLOOccluder = None
    print("âš ï¸ YOLO æœªå®‰è£ï¼Œç‰©ä»¶é®æ“‹åŠŸèƒ½ä¸å¯ç”¨")


class CompleteDemoVisualizer:
    """TempoVLM å®Œæ•´å±•ç¤ºè¦–è¦ºåŒ–å™¨"""
    
    def __init__(self, unified_model_path, device='cuda'):
        self.device = device
        self.checkpoint_path = unified_model_path  # è¨˜éŒ„ä½¿ç”¨çš„ checkpoint
        
        print("=" * 70)
        print("TempoVLM Complete Demo Visualizer")
        print("=" * 70)
        print(f"\nğŸ“¦ ä½¿ç”¨ Checkpoint: {unified_model_path}")
        
        # è¼‰å…¥æ¨¡å‹
        print("\nloading...")
        self.processor = AutoProcessor.from_pretrained(
            "Qwen/Qwen2-VL-2B-Instruct",
            trust_remote_code=True
        )
        self.base_model = Qwen2VLForConditionalGeneration.from_pretrained(
            "Qwen/Qwen2-VL-2B-Instruct",
            torch_dtype=torch.float16,
            device_map=device,
            trust_remote_code=True
        ).eval()
        
        self._load_unified_model(unified_model_path)
        
        # æ™‚åºç·©è¡å€
        self.temporal_buffer = deque(maxlen=5)
        
        # ç‰¹å¾µæŠ•å½±å™¨ï¼ˆç”¨æ–¼æ³¨å…¥æ™‚ç¶­åº¦è½‰æ›ï¼‰
        self.feature_projector = None
        
        print("model loaded.\n")
    
    def _load_unified_model(self, model_path):
        from models_unified import UnifiedTempoVLM
        
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
        state_dict = checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint
        
        # è‡ªå‹•åµæ¸¬æ¨¡å‹åƒæ•¸
        if 'shared_encoder.0.weight' in state_dict:
            hidden_dim = state_dict['shared_encoder.0.weight'].shape[0]
        else:
            hidden_dim = 768
        
        # åµæ¸¬æ˜¯å¦ä½¿ç”¨ GRU
        use_gru = 'temporal_gru.weight_ih' in state_dict or 'temporal_gru.weight_hh' in state_dict
        
        self.unified_model = UnifiedTempoVLM(
            hidden_dim=hidden_dim,
            use_gru_memory=use_gru
        ).to(self.device)
        
        if 'model_state_dict' in checkpoint:
            state_dict_to_load = checkpoint['model_state_dict']
        else:
            state_dict_to_load = checkpoint
        
        # ğŸ”§ è™•ç†èˆŠ checkpoint çš„ memory_quality_gate æ¶æ§‹ä¸åŒ¹é…å•é¡Œ
        # èˆŠç‰ˆ: 3 å±¤ (0: Linear, 1: GELU, 2: Linear)
        # æ–°ç‰ˆ: 4 å±¤ (0: Linear, 1: GELU, 2: Dropout, 3: Linear)
        if 'memory_quality_gate.2.weight' in state_dict_to_load and \
           'memory_quality_gate.3.weight' not in state_dict_to_load:
            print("  âš ï¸ åµæ¸¬åˆ°èˆŠç‰ˆ checkpointï¼Œæ­£åœ¨é·ç§» memory_quality_gate æ¶æ§‹...")
            # å°‡èˆŠçš„ layer 2 (æœ€å¾Œçš„ Linear) ç§»åˆ° layer 3
            state_dict_to_load['memory_quality_gate.3.weight'] = state_dict_to_load.pop('memory_quality_gate.2.weight')
            state_dict_to_load['memory_quality_gate.3.bias'] = state_dict_to_load.pop('memory_quality_gate.2.bias')
            print("  âœ… æ¶æ§‹é·ç§»å®Œæˆ (Dropout å±¤ä½¿ç”¨é è¨­åˆå§‹åŒ–)")
        
        self.unified_model.load_state_dict(state_dict_to_load, strict=False)
        
        self.unified_model.eval()
        self.unified_model.float()
        self.hidden_dim = hidden_dim
        self.use_gru = use_gru
        
        self.gru_hidden_state = None
        
        print(f"  âœ… Unified Model loaded (hidden_dim={hidden_dim}, GRU={use_gru})")
    
    def extract_features(self, image, use_adapter=True):
        messages = [{
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": "Describe."}
            ]
        }]
        
        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = self.processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt"
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.base_model(
                **inputs,
                output_hidden_states=True,
                return_dict=True
            )
            hidden_states = outputs.hidden_states[-1]
            features = hidden_states.mean(dim=1).float()
        
        if use_adapter and self.unified_model is not None:
            self.temporal_buffer.append(features)
            if len(self.temporal_buffer) >= 2:
                prev_feat = self.temporal_buffer[-2]
                with torch.no_grad():
                    if hasattr(self, 'use_gru') and self.use_gru:
                        outputs, self.gru_hidden_state = self.unified_model(
                            features, prev_feat, 
                            hidden_state=self.gru_hidden_state,
                            tasks=['temporal']
                        )
                    else:
                        outputs, _ = self.unified_model(features, prev_feat, tasks=['temporal'])
                    features = outputs['temporal']
        
        return features

    def extract_edge_features(self, image):
        """
        æå–é‚Šç·£ç‰¹å¾µ (ç”¨æ–¼ v6.1 Scene Change Detection)
        
        æ–¹æ³•: å°‡åœ–ç‰‡ä¸­å¿ƒ 60% å€åŸŸå¡—é»‘ï¼Œåªä¿ç•™é‚Šç·£ï¼Œç„¶å¾Œæå–ç‰¹å¾µã€‚
        é€™æ¨£å¼·åˆ¶æ¨¡å‹åªçœ‹å‘¨åœç’°å¢ƒ (ç‰†å£ã€å¤©èŠ±æ¿ã€åœ°æ¿)ï¼Œå¿½ç•¥ä¸­å¿ƒç‰©é«”ã€‚
        """
        import numpy as np
        from PIL import Image as PILImage
        
        if isinstance(image, PILImage.Image):
            img_array = np.array(image).copy()
        else:
            img_array = image.copy()
            
        h, w = img_array.shape[:2]
        
        # å®šç¾©é®ç½©å€åŸŸ (ä¿ç•™é‚Šç·£ 20%)
        margin_h = int(h * 0.2)
        margin_w = int(w * 0.2)
        
        # å°‡ä¸­å¿ƒå€åŸŸå¡—é»‘
        img_array[margin_h:h-margin_h, margin_w:w-margin_w] = 0
        
        masked_image = PILImage.fromarray(img_array)
        
        # æå–ç‰¹å¾µ (ä¸ä½¿ç”¨ Adapterï¼Œåªå–ç´”è¦–è¦ºç‰¹å¾µ)
        return self.extract_features(masked_image, use_adapter=False)
    
    def generate_description(self, image, prompt="Describe what you see in the center of this image."):
        messages = [{
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": prompt}
            ]
        }]
        
        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = self.processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt"
        ).to(self.device)
        
        with torch.no_grad():
            generated = self.base_model.generate(
                **inputs,
                max_new_tokens=150,
                do_sample=False
            )
        
        response = self.processor.decode(generated[0], skip_special_tokens=True)
        if "assistant" in response.lower():
            response = response.split("assistant")[-1].strip()
        return response
    
    def detect_occlusion_regions(self, image):
        """
        ğŸ” è‡ªå‹•åµæ¸¬åœ–åƒä¸­çš„é®æ“‹å€åŸŸ
        
        Args:
            image: PIL Image æˆ– numpy array
            
        Returns:
            occlusion_mask: (H, W) çš„ numpy arrayï¼Œé®æ“‹å€åŸŸç‚º 1ï¼Œå…¶ä»–ç‚º 0
        """
        if isinstance(image, Image.Image):
            img_array = np.array(image)
        else:
            img_array = image
        
        if img_array.shape[-1] == 3:
            # RGB åœ–åƒ
            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
        else:
            gray = img_array
        
        h, w = gray.shape
        
        # æ–¹æ³•1: æª¢æ¸¬ç´”é»‘è‰²å€åŸŸ (é®æ“‹å¸¸ç”¨é»‘è‰²)
        black_mask = (gray < 10).astype(np.uint8)
        
        # æ–¹æ³•2: æª¢æ¸¬ä½ç´‹ç†å€åŸŸï¼ˆé®æ“‹é€šå¸¸æ˜¯å‡å‹»çš„ï¼‰
        laplacian = cv2.Laplacian(gray, cv2.CV_64F)
        low_texture_mask = (np.abs(laplacian) < 5).astype(np.uint8)
        
        # çµåˆå…©ç¨®æ–¹æ³•
        combined_mask = np.logical_and(black_mask, low_texture_mask).astype(np.uint8)
        
        # å½¢æ…‹å­¸æ“ä½œï¼šå»é™¤å™ªé»ã€å¡«å……å°å­”
        kernel = np.ones((5, 5), np.uint8)
        combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_CLOSE, kernel)
        combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_OPEN, kernel)
        
        # åªä¿ç•™è¼ƒå¤§çš„é€£é€šå€åŸŸï¼ˆéæ¿¾å°å™ªé»ï¼‰
        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(combined_mask, connectivity=8)
        
        filtered_mask = np.zeros_like(combined_mask)
        min_area = (h * w) * 0.01  # è‡³å°‘ä½” 1% é¢ç©
        
        for i in range(1, num_labels):  # è·³éèƒŒæ™¯ (label 0)
            area = stats[i, cv2.CC_STAT_AREA]
            if area > min_area:
                filtered_mask[labels == i] = 1
        
        return filtered_mask
    
    def generate_with_injection(self, image, memory_feat, prompt, injection_strength=0.5, injection_method='full', 
                               occlusion_info=None):
        """
        ğŸ§  Direct Feature Injection - è‡ªå‹•åµæ¸¬é®æ“‹å€åŸŸä¸¦é‡å°æ€§æ³¨å…¥
        
        å°‡è¨˜æ†¶ç‰¹å¾µæ³¨å…¥åˆ°è¦–è¦ºç·¨ç¢¼å™¨è¼¸å‡ºä¸­
        
        Args:
            image: ç•¶å‰å¹€ï¼ˆå¯èƒ½è¢«é®æ“‹ï¼‰
            memory_feat: è¦æ³¨å…¥çš„è¨˜æ†¶ç‰¹å¾µ
            prompt: æå•
            injection_strength: æ³¨å…¥å¼·åº¦ (0-1)
            injection_method: 'raw', 'full', 'strong', 'adaptive'
            occlusion_info: é®æ“‹ç‰©ä»¶è³‡è¨Š (ä¾†è‡ª YOLO)ï¼ŒåŒ…å« bbox
        
        Returns:
            str: ç”Ÿæˆçš„å›ç­”
        """
        messages = [{
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": prompt}
            ]
        }]
        
        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = self.processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt"
        ).to(self.device)
        
        # è¤‡è£½è¨˜æ†¶ç‰¹å¾µ
        enhanced_feat_copy = memory_feat.clone().detach()
        
        # åˆå§‹åŒ–ç‰¹å¾µæŠ•å½±å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        vision_hidden_size = self.base_model.visual.config.hidden_size if hasattr(self.base_model.visual, 'config') else 1536
        enhanced_dim = enhanced_feat_copy.shape[-1]
        
        if enhanced_dim != vision_hidden_size:
            if not hasattr(self, 'feature_projector') or self.feature_projector is None:
                self.feature_projector = torch.nn.Linear(enhanced_dim, vision_hidden_size)
                torch.nn.init.eye_(self.feature_projector.weight[:min(enhanced_dim, vision_hidden_size), :min(enhanced_dim, vision_hidden_size)])
                torch.nn.init.zeros_(self.feature_projector.bias)
                self.feature_projector = self.feature_projector.to(self.device).half()
        
        # ğŸ” è‡ªå‹•åµæ¸¬æˆ–ä½¿ç”¨æä¾›çš„é®æ“‹å€åŸŸè³‡è¨Š
        occlusion_mask_2d = None
        if occlusion_info and 'objects' in occlusion_info:
            # ä½¿ç”¨ YOLO æä¾›çš„ bbox ç”Ÿæˆé®ç½©
            img_array = np.array(image) if isinstance(image, Image.Image) else image
            h, w = img_array.shape[:2]
            occlusion_mask_2d = np.zeros((h, w), dtype=np.float32)
            
            for obj in occlusion_info['objects']:
                x1, y1, x2, y2 = obj['bbox']
                # æ“´å±• bbox å‘¨åœå€åŸŸï¼ˆè£œå„Ÿé®æ“‹å½±éŸ¿ç¯„åœï¼‰
                margin = 20
                x1 = max(0, x1 - margin)
                y1 = max(0, y1 - margin)
                x2 = min(w, x2 + margin)
                y2 = min(h, y2 + margin)
                occlusion_mask_2d[y1:y2, x1:x2] = 1.0
        else:
            # è‡ªå‹•åµæ¸¬é®æ“‹å€åŸŸ
            occlusion_mask_2d = self.detect_occlusion_regions(image).astype(np.float32)
        
        # å°‡é®ç½©è½‰æ›ç‚º Tensor ä¾› hook ä½¿ç”¨
        occlusion_mask_tensor = torch.from_numpy(occlusion_mask_2d).to(self.device)
        
        def create_injection_hook(method, strength, occl_mask):
            def injection_hook(module, input, output):
                nonlocal enhanced_feat_copy
                
                with torch.no_grad():
                    # æŠ•å½±ç‰¹å¾µåˆ°è¦–è¦ºç·¨ç¢¼å™¨çš„ç¶­åº¦
                    if enhanced_dim != vision_hidden_size:
                        projected = self.feature_projector(enhanced_feat_copy.float()).half()
                    else:
                        projected = enhanced_feat_copy
                    
                    # æ“´å±•åˆ°æ­£ç¢ºçš„å½¢ç‹€
                    if output.dim() == 2:
                        num_patches = output.shape[0]
                        projected_expanded = projected.squeeze(0).unsqueeze(0).expand(num_patches, -1)
                        batch = 1
                    elif output.dim() == 3:
                        batch, num_patches, _ = output.shape
                        projected_expanded = projected.unsqueeze(1).expand(batch, num_patches, -1)
                    else:
                        return output
                    
                    # ğŸ”‘ æ­£è¦åŒ–ï¼šå„ªå…ˆç”¨è¨˜æ†¶çµ±è¨ˆï¼Œé¿å…é®æ“‹åœ–çš„ä½æ–¹å·®æŠŠè¨˜æ†¶å£“æ‰
                    orig_mean = output.mean()
                    orig_std = output.std() + 1e-6
                    proj_mean = projected_expanded.mean()
                    proj_std = projected_expanded.std() + 1e-6
                    
                    # ğŸ”¥ æ›´æ¿€é€²çš„è¨˜æ†¶å„ªå…ˆç­–ç•¥ï¼ˆæé«˜è¨˜æ†¶ç‰¹å¾µçš„å½±éŸ¿åŠ›ï¼‰
                    blended_mean = 0.4 * proj_mean + 0.6 * orig_mean  # å¾ 0.5/0.5 æ”¹ç‚º 0.4/0.6
                    blended_std = torch.max(0.8 * proj_std + 0.2 * orig_std, 0.6 * proj_std)  # å¾ 0.7/0.3 æ”¹ç‚º 0.8/0.2
                    projected_normalized = (projected_expanded - proj_mean) / proj_std * blended_std + blended_mean
                    
                    # ğŸ¯ å‹•æ…‹é®æ“‹é®ç½©ï¼šæ ¹æ“šå¯¦éš›é®æ“‹å€åŸŸç”Ÿæˆ patch ç´šåˆ¥çš„é®ç½©
                    if num_patches > 4:
                        side = int(num_patches ** 0.5)
                        if side * side == num_patches and occl_mask is not None:
                            # å°‡ 2D é®æ“‹é®ç½©é™æ¡æ¨£åˆ° patch ç¶²æ ¼
                            h, w = occl_mask.shape
                            patch_h = h // side
                            patch_w = w // side
                            
                            # ç‚ºæ¯å€‹ patch è¨ˆç®—é®æ“‹æ¯”ä¾‹
                            injection_mask = torch.zeros((1, num_patches, 1), device=output.device)
                            for i in range(side):
                                for j in range(side):
                                    patch_idx = i * side + j
                                    y_start = i * patch_h
                                    y_end = (i + 1) * patch_h if i < side - 1 else h
                                    x_start = j * patch_w
                                    x_end = (j + 1) * patch_w if j < side - 1 else w
                                    
                                    # è¨ˆç®—è©² patch çš„é®æ“‹æ¯”ä¾‹
                                    patch_region = occl_mask[y_start:y_end, x_start:x_end]
                                    occlusion_ratio = patch_region.mean().item()
                                    
                                    # é®æ“‹æ¯”ä¾‹è¶Šé«˜ï¼Œæ³¨å…¥å¼·åº¦è¶Šå¤§
                                    # ä¸¦æ“´å±•å½±éŸ¿åˆ°é„°è¿‘ patchï¼ˆè£œå„Ÿé®æ“‹é‚Šç•Œæ•ˆæ‡‰ï¼‰
                                    injection_mask[0, patch_idx, 0] = occlusion_ratio
                            
                            # å¹³æ»‘é®ç½©ï¼šä½¿ç”¨é„°åŸŸå¹³å‡ï¼Œè®“æ³¨å…¥æ›´è‡ªç„¶
                            if side >= 3:
                                mask_2d = injection_mask.view(1, side, side, 1)
                                kernel_size = 3
                                padding = kernel_size // 2
                                mask_2d_padded = F.pad(mask_2d.permute(0, 3, 1, 2), 
                                                       (padding, padding, padding, padding), 
                                                       mode='replicate')
                                smoothed = F.avg_pool2d(mask_2d_padded, kernel_size, stride=1, padding=0)
                                injection_mask = smoothed.permute(0, 2, 3, 1).reshape(1, num_patches, 1)
                            
                            # ğŸ”¥ å¤§å¹…å¢å¼·é®æ“‹å€åŸŸçš„æ³¨å…¥å¼·åº¦ï¼ˆå¾ 1.8 æé«˜åˆ° 2.5ï¼‰
                            # é®æ“‹å€åŸŸéœ€è¦æ›´å¼·çš„è¨˜æ†¶æ³¨å…¥æ‰èƒ½æ¢å¾©
                            injection_mask = torch.clamp(injection_mask * 2.5, max=1.0)
                            
                            # ğŸ¯ å°é®æ“‹æ¯”ä¾‹ > 50% çš„ patch å†æ¬¡åŠ å¼·
                            high_occlusion = (injection_mask > 0.5).float()
                            injection_mask = injection_mask + high_occlusion * 0.2
                            injection_mask = torch.clamp(injection_mask, max=1.0)
                        else:
                            # Fallback: ä½¿ç”¨ä¸­å¿ƒé®ç½©ï¼ˆå‚³çµ±æ–¹æ³•ï¼‰
                            idxs = torch.arange(num_patches, device=output.device).view(1, num_patches, 1)
                            rows = (idxs // side).float()
                            cols = (idxs % side).float()
                            center = (side - 1) / 2
                            dist = torch.maximum((rows - center).abs(), (cols - center).abs()) / (side / 2)
                            injection_mask = (dist < 0.8).float()
                            injection_mask = torch.clamp(injection_mask * 1.5, max=1.0)
                    else:
                        injection_mask = torch.ones((1, num_patches, 1), device=output.device)
                    
                    # ============================================================
                    # æ³¨å…¥æ–¹æ³•é¸æ“‡ (å’Œ occlusion_tester.py ç›¸åŒ)
                    # ============================================================
                    
                    if method == 'full':
                        # æ–¹æ³•1: å…¨åœ–æ³¨å…¥ (ä½¿ç”¨å‹•æ…‹é®æ“‹é®ç½©)
                        mix = strength * injection_mask
                        if output.dim() == 3:
                            modified = output + mix * (projected_normalized - output)
                        else:
                            modified = output + mix.squeeze(0) * (projected_normalized - output)
                    
                    elif method == 'strong':
                        # æ–¹æ³•2: å¼·åŠ›é®æ“‹å€åŸŸæ³¨å…¥
                        modified = output.clone()
                        if output.dim() == 3:
                            batch_size, num_patches, _ = output.shape
                            side = int(num_patches ** 0.5)
                            if side * side == num_patches:
                                for row in range(side):
                                    for col in range(side):
                                        idx = row * side + col
                                        # ä½¿ç”¨é®æ“‹é®ç½©æ¬Šé‡
                                        local_strength = strength * injection_mask[:, idx, :].squeeze(-1)
                                        modified[:, idx] = (1 - local_strength) * output[:, idx] + local_strength * projected_normalized[:, idx]
                            else:
                                mix = strength * injection_mask
                                modified = output + mix * (projected_normalized - output)
                        else:
                            mix = strength * injection_mask
                            modified = output + mix.squeeze(0) * (projected_normalized - output)
                    
                    elif method == 'adaptive':
                        # æ–¹æ³•3: è‡ªé©æ‡‰æ³¨å…¥ - çµåˆç‰¹å¾µå·®ç•°å’Œé®æ“‹é®ç½©
                        if output.dim() == 3:
                            diff = torch.abs(output - projected_normalized).mean(dim=-1, keepdim=True)
                            diff_normalized = diff / (diff.max() + 1e-6)
                            # ğŸ”¥ æ›´æ¿€é€²çš„è‡ªé©æ‡‰ç­–ç•¥ï¼šé®æ“‹å€åŸŸ + é«˜å·®ç•°å€åŸŸ
                            # å¾ (0.5 + 0.5 * diff) æ”¹ç‚º (0.3 + 0.7 * diff)ï¼Œè®“å·®ç•°å½±éŸ¿æ›´å¤§
                            adaptive_strength = strength * (0.3 + 0.7 * diff_normalized) * injection_mask
                            # åœ¨é®æ“‹å€åŸŸé¡å¤–å¢å¼· 20%
                            occlusion_boost = injection_mask * 0.2
                            adaptive_strength = torch.clamp(adaptive_strength + occlusion_boost, max=1.0)
                            modified = (1 - adaptive_strength) * output + adaptive_strength * projected_normalized
                        else:
                            diff = torch.abs(output - projected_normalized).mean(dim=-1, keepdim=True)
                            diff_normalized = diff / (diff.max() + 1e-6)
                            adaptive_strength = strength * (0.3 + 0.7 * diff_normalized) * injection_mask.squeeze(0)
                            occlusion_boost = injection_mask.squeeze(0) * 0.2
                            adaptive_strength = torch.clamp(adaptive_strength + occlusion_boost, max=1.0)
                            modified = (1 - adaptive_strength) * output + adaptive_strength * projected_normalized
                    
                    else:  # 'raw' æˆ–å…¶ä»–
                        # æ–¹æ³•4: åŸå§‹é®æ“‹å€åŸŸæ³¨å…¥ï¼ˆä¿å®ˆï¼‰
                        mix = strength * injection_mask
                        if output.dim() == 3:
                            modified = output + mix * (projected_normalized - output)
                        else:
                            modified = output + mix.squeeze(0) * (projected_normalized - output)
                    
                    # é™åˆ¶æ•¸å€¼ç¯„åœï¼Œé˜²æ­¢æ¥µç«¯å€¼
                    modified = torch.clamp(modified, orig_mean - 4*orig_std, orig_mean + 4*orig_std)
                    return modified
            
            return injection_hook
        
        hook_handle = self.base_model.visual.register_forward_hook(
            create_injection_hook(injection_method, injection_strength, occlusion_mask_tensor)
        )
        
        try:
            with torch.no_grad():
                generated = self.base_model.generate(
                    **inputs,
                    max_new_tokens=150,
                    do_sample=False
                )
        finally:
            hook_handle.remove()
        
        # è§£ç¢¼ç”Ÿæˆçš„æ–‡æœ¬
        full_response = self.processor.decode(generated[0], skip_special_tokens=True)
        
        # æå– assistant å›ç­”éƒ¨åˆ†
        response = full_response
        separators = ['assistant\n', 'assistant:', 'Assistant:', 'ASSISTANT:', '<|assistant|>']
        for sep in separators:
            if sep.lower() in response.lower():
                idx = response.lower().find(sep.lower())
                response = response[idx + len(sep):].strip()
                break
        
        return response
    
    def clear_temporal_buffer(self):
        """æ¸…é™¤æ™‚åºç·©è¡å€å’Œ GRU éš±è—ç‹€æ…‹"""
        self.temporal_buffer.clear()
        if hasattr(self, 'gru_hidden_state'):
            self.gru_hidden_state = None
    
    # ========== 1. æ™‚åºä¸€è‡´æ€§è¦–è¦ºåŒ– ==========
    
    def visualize_temporal_consistency(self, scene_dir, output_path, max_frames=60):
        """ç”Ÿæˆæ™‚åºä¸€è‡´æ€§å°æ¯”å½±ç‰‡"""
        print("\nCreating Temporal Consistency Video...")
        
        color_dir = scene_dir / 'color'
        frame_files = sorted(color_dir.glob('*.jpg'))[:max_frames]
        
        if len(frame_files) < 10:
            print("frame count insufficient.")
            return None
        
        self.clear_temporal_buffer()
        

        print("  feature extraction...")
        base_features = []
        for f in tqdm(frame_files, desc="  Base"):
            img = Image.open(f).convert('RGB')
            feat = self.extract_features(img, use_adapter=False)
            base_features.append(feat)
        
        self.clear_temporal_buffer()
        unified_features = []
        for i, f in enumerate(tqdm(frame_files, desc="  Unified")):
            img = Image.open(f).convert('RGB')
            feat = self.extract_features(img, use_adapter=True)
            unified_features.append(feat)
        
        # calculate similarities
        base_sims = [1.0]
        unified_sims = [1.0]
        for i in range(1, len(base_features)):
            base_sim = F.cosine_similarity(base_features[i], base_features[i-1], dim=-1).item()
            unified_sim = F.cosine_similarity(unified_features[i], unified_features[i-1], dim=-1).item()
            base_sims.append(base_sim)
            unified_sims.append(unified_sim)
        
        # ç”Ÿæˆå½±ç‰‡
        print("  ç”Ÿæˆå½±ç‰‡...")
        
        frame_width = 1280
        frame_height = 720
        fps = 10
        
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(str(output_path), fourcc, fps, (frame_width, frame_height))
        
        for i, frame_file in enumerate(tqdm(frame_files, desc="  å¯«å…¥å¹€")):
            img = cv2.imread(str(frame_file))
            img = cv2.resize(img, (640, 480))
            
            canvas = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)
            canvas[:] = (30, 30, 30)
            
            canvas[20:500, 20:660] = img
            
            # ç¹ªè£½ç›¸ä¼¼åº¦æ›²ç·š
            fig, ax = plt.subplots(figsize=(6, 3), facecolor='#1e1e1e')
            ax.set_facecolor('#1e1e1e')
            
            x = np.arange(i + 1)
            ax.plot(x, base_sims[:i+1], 'r-', label='Base Model', linewidth=2)
            ax.plot(x, unified_sims[:i+1], 'g-', label='Unified Model', linewidth=2)
            
            ax.set_xlim(0, len(frame_files))
            ax.set_ylim(0.8, 1.0)
            ax.set_xlabel('Frame', color='white')
            ax.set_ylabel('Cosine Similarity', color='white')
            ax.set_title('Temporal Feature Consistency', color='white')
            ax.legend(loc='lower left', facecolor='#2e2e2e', edgecolor='white', labelcolor='white')
            ax.tick_params(colors='white')
            ax.grid(True, alpha=0.3)
            
            for spine in ax.spines.values():
                spine.set_edgecolor('white')
            
            fig.tight_layout()
            fig.canvas.draw()
            
            plot_img = np.array(fig.canvas.buffer_rgba())[:, :, :3]
            plot_img = cv2.cvtColor(plot_img, cv2.COLOR_RGB2BGR)
            plot_img = cv2.resize(plot_img, (600, 200))
            plt.close(fig)
            
            canvas[510:710, 20:620] = plot_img
            
            # å³å´é¢æ¿
            current_base_sim = base_sims[i]
            current_unified_sim = unified_sims[i]
            
            panel_x = 680
            cv2.putText(canvas, f'Frame: {i+1}/{len(frame_files)}', (panel_x, 50),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
            
            cv2.putText(canvas, 'Current Similarity:', (panel_x, 100),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            cv2.putText(canvas, f'Base:    {current_base_sim:.4f}', (panel_x, 140),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (100, 100, 255), 2)
            
            cv2.putText(canvas, f'Unified: {current_unified_sim:.4f}', (panel_x, 180),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (100, 255, 100), 2)
            
            improve = (current_unified_sim - current_base_sim) / max(current_base_sim, 0.001) * 100
            color = (100, 255, 100) if improve > 0 else (100, 100, 255)
            cv2.putText(canvas, f'Improvement: {improve:+.2f}%', (panel_x, 230),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
            
            cv2.putText(canvas, 'TempoVLM: Temporal Consistency Demo', (20, frame_height - 20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (150, 150, 150), 1)
            
            out.write(canvas)
        
        out.release()
        print(f"âœ… å½±ç‰‡å·²ä¿å­˜: {output_path}")
        
        # è¿”å›çµ±è¨ˆ
        return {
            'base_mean_sim': float(np.mean(base_sims)),
            'unified_mean_sim': float(np.mean(unified_sims)),
            'improvement': float((np.mean(unified_sims) - np.mean(base_sims)) / np.mean(base_sims) * 100)
        }
    
    # ========== 2. æ·±åº¦æ’åºè¦–è¦ºåŒ– ==========
    
    def visualize_depth_ordering(self, scene_dir, output_path, max_frames=60):
        """ç”Ÿæˆæ·±åº¦æ’åºèƒ½åŠ›å±•ç¤ºå½±ç‰‡"""
        print("\nğŸ¬ ç”Ÿæˆæ·±åº¦æ’åºèƒ½åŠ›å±•ç¤º...")
        
        color_dir = scene_dir / 'color'
        depth_dir = scene_dir / 'depth'
        
        frame_files = sorted(color_dir.glob('*.jpg'))[:max_frames]
        
        if len(frame_files) < 10:
            print("âŒ å¹€æ•¸ä¸è¶³")
            return None
        
        frame_width = 1280
        frame_height = 720
        fps = 10
        
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(str(output_path), fourcc, fps, (frame_width, frame_height))
        
        correct_predictions = 0
        total_predictions = 0
        
        for i, frame_file in enumerate(tqdm(frame_files, desc="  ç”Ÿæˆå¹€")):
            img_pil = Image.open(frame_file).convert('RGB')
            img_cv = cv2.imread(str(frame_file))
            img_resized = cv2.resize(img_cv, (640, 360))
            
            depth_file = depth_dir / (frame_file.stem + '.png')
            if depth_file.exists():
                depth_raw = cv2.imread(str(depth_file), cv2.IMREAD_UNCHANGED)
                depth = depth_raw.astype(np.float32) / 1000.0
            else:
                depth = np.ones((480, 640)) * 5.0
            
            depth_resized = cv2.resize(depth, (640, 360))
            
            # è¨ˆç®—ä¸‰å€‹å€åŸŸçš„æ·±åº¦
            h, w = depth_resized.shape
            gt_depths = {}
            depth_regions = {
                'left': depth_resized[h//4:3*h//4, :w//3],
                'center': depth_resized[h//4:3*h//4, w//3:2*w//3],
                'right': depth_resized[h//4:3*h//4, 2*w//3:],
            }
            
            for name, region in depth_regions.items():
                valid = region[(region > 0.1) & (region < 10)]
                gt_depths[name] = valid.mean() if len(valid) > 0 else 5.0
            
            # å‰µå»ºç•«å¸ƒ
            canvas = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)
            canvas[:] = (30, 30, 30)
            
            # åŸå§‹åœ– + å€åŸŸæ¨™è¨˜
            img_with_regions = img_resized.copy()
            h_vis, w_vis = img_with_regions.shape[:2]
            y1, y2 = h_vis//4, 3*h_vis//4
            
            cv2.rectangle(img_with_regions, (0, y1), (w_vis//3, y2), (100, 100, 255), 2)
            cv2.putText(img_with_regions, f'L:{gt_depths["left"]:.1f}m', (5, y1 + 20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 100, 255), 2)
            
            cv2.rectangle(img_with_regions, (w_vis//3, y1), (2*w_vis//3, y2), (100, 255, 100), 2)
            cv2.putText(img_with_regions, f'C:{gt_depths["center"]:.1f}m', (w_vis//3 + 5, y1 + 20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 255, 100), 2)
            
            cv2.rectangle(img_with_regions, (2*w_vis//3, y1), (w_vis, y2), (255, 100, 100), 2)
            cv2.putText(img_with_regions, f'R:{gt_depths["right"]:.1f}m', (2*w_vis//3 + 5, y1 + 20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 100, 100), 2)
            
            canvas[20:380, 20:660] = img_with_regions
            
            # æ·±åº¦åœ–è¦–è¦ºåŒ–
            depth_vis = cv2.applyColorMap(
                (np.clip(depth_resized / 5.0, 0, 1) * 255).astype(np.uint8),
                cv2.COLORMAP_JET
            )
            canvas[20:380, 680:1260] = cv2.resize(depth_vis, (580, 360))
            
            # çµ±è¨ˆé¢æ¿
            cv2.putText(canvas, f'Frame: {i+1}/{len(frame_files)}', (20, 420),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            cv2.putText(canvas, f'Depth Ordering Demo', (20, 460),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)
            
            out.write(canvas)
        
        out.release()
        print(f"âœ… å½±ç‰‡å·²ä¿å­˜: {output_path}")
        
        return {
            'total_frames': len(frame_files)
        }
    
    # ========== 2.5 æ·±åº¦å›æ­¸è¦–è¦ºåŒ– (NEW) ==========
    
    def visualize_depth_regression(self, scene_dir, output_path, max_frames=60):
        """
        ç”Ÿæˆæ·±åº¦å›æ­¸é æ¸¬ vs Ground Truth çš„æ¯”è¼ƒå½±ç‰‡
        
        é¡¯ç¤ºå…§å®¹:
        1. åŸå§‹ RGB åœ–åƒ + å€åŸŸæ¨™è¨˜
        2. GT æ·±åº¦åœ– (ç†±åŠ›åœ–)
        3. ä¸‰å€‹å€åŸŸ (å·¦/ä¸­/å³) çš„é æ¸¬æ·±åº¦ vs GT æ·±åº¦æŸ±ç‹€åœ–
        4. é æ¸¬èª¤å·®æ›²ç·š
        5. è©•åˆ†ç­‰ç´š
        """
        print("\nğŸ¬ ç”Ÿæˆæ·±åº¦å›æ­¸æ¯”è¼ƒå½±ç‰‡...")
        
        # æª¢æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æ´ depth_regression
        if not hasattr(self.unified_model, 'depth_regression_head'):
            print("âš ï¸ æ¨¡å‹æœªåŒ…å« depth_regression_headï¼Œä½¿ç”¨æ¨¡æ“¬é æ¸¬")
            use_mock = True
        else:
            use_mock = False
        
        color_dir = scene_dir / 'color'
        depth_dir = scene_dir / 'depth'
        
        frame_files = sorted(color_dir.glob('*.jpg'))[:max_frames]
        
        if len(frame_files) < 10:
            print("âŒ å¹€æ•¸ä¸è¶³")
            return None
        
        # å½±ç‰‡åƒæ•¸
        frame_width = 1280
        frame_height = 720
        fps = 10
        
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(str(output_path), fourcc, fps, (frame_width, frame_height))
        
        # çµ±è¨ˆ
        errors_left = []
        errors_center = []
        errors_right = []
        all_preds = {'left': [], 'center': [], 'right': []}
        all_gts = {'left': [], 'center': [], 'right': []}
        
        for i, frame_file in enumerate(tqdm(frame_files, desc="  ç”Ÿæˆå¹€")):
            # è®€å–åœ–åƒ
            img_pil = Image.open(frame_file).convert('RGB')
            img_cv = cv2.imread(str(frame_file))
            img_resized = cv2.resize(img_cv, (640, 360))
            
            # è®€å–æ·±åº¦
            depth_file = depth_dir / (frame_file.stem + '.png')
            if depth_file.exists():
                depth_raw = cv2.imread(str(depth_file), cv2.IMREAD_UNCHANGED)
                depth = depth_raw.astype(np.float32) / 1000.0  # mm to m
            else:
                depth = np.ones((480, 640)) * 3.0
            
            depth_resized = cv2.resize(depth, (640, 360))
            
            # å®šç¾©ä¸‰å€‹å€åŸŸ
            img_w, img_h = img_pil.size
            region_width = img_w // 3
            region_height = img_h // 2
            y_start = img_h // 4
            
            regions = {
                'left': img_pil.crop((0, y_start, region_width, y_start + region_height)),
                'center': img_pil.crop((region_width, y_start, 2*region_width, y_start + region_height)),
                'right': img_pil.crop((2*region_width, y_start, img_w, y_start + region_height)),
            }
            
            # è¨ˆç®— GT æ·±åº¦
            h, w = depth_resized.shape
            depth_regions = {
                'left': depth_resized[h//4:3*h//4, :w//3],
                'center': depth_resized[h//4:3*h//4, w//3:2*w//3],
                'right': depth_resized[h//4:3*h//4, 2*w//3:],
            }
            
            gt_depths = {}
            for name, region in depth_regions.items():
                valid = region[(region > 0.1) & (region < 10)]
                if len(valid) > 0:
                    gt_depths[name] = valid.mean()
                else:
                    gt_depths[name] = 3.0
            
            # é æ¸¬æ·±åº¦
            pred_depths = {}
            if use_mock:
                # æ¨¡æ“¬é æ¸¬ï¼šGT + éš¨æ©Ÿå™ªè²
                for name in ['left', 'center', 'right']:
                    noise = np.random.randn() * 0.5
                    pred_depths[name] = max(0.5, min(5.0, gt_depths[name] + noise))
            else:
                with torch.no_grad():
                    for name, crop in regions.items():
                        crop_resized = crop.resize((224, 224))
                        feat = self.extract_features(crop_resized)
                        
                        # ä½¿ç”¨æ–° API é€²è¡Œæ·±åº¦å›æ­¸
                        outputs, _ = self.unified_model(feat.float(), tasks=['depth_regression'])
                        pred_depth_raw = outputs['depth_regression'].squeeze()
                        
                        # å–å°æ‡‰å€åŸŸçš„æ·±åº¦
                        if name == 'left':
                            pred_depth = pred_depth_raw[0].item()
                        elif name == 'center':
                            pred_depth = pred_depth_raw[1].item()
                        else:  # right
                            pred_depth = pred_depth_raw[2].item()
                        
                        pred_depth = max(0.5, min(10.0, pred_depth))
                        pred_depths[name] = pred_depth
            
            # è¨ˆç®—èª¤å·®
            for name in ['left', 'center', 'right']:
                error = abs(pred_depths[name] - gt_depths[name])
                if name == 'left':
                    errors_left.append(error)
                elif name == 'center':
                    errors_center.append(error)
                else:
                    errors_right.append(error)
                
                all_preds[name].append(pred_depths[name])
                all_gts[name].append(gt_depths[name])
            
            # ========== ç¹ªè£½ç•«å¸ƒ ==========
            canvas = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)
            canvas[:] = (30, 30, 30)
            
            # ========== å·¦ä¸Š: RGB åœ– + å€åŸŸæ¨™è¨˜ ==========
            img_with_regions = img_resized.copy()
            h_vis, w_vis = img_with_regions.shape[:2]
            y1, y2 = h_vis//4, 3*h_vis//4
            
            colors = {'left': (100, 100, 255), 'center': (100, 255, 100), 'right': (255, 100, 100)}
            boxes = {
                'left': (0, y1, w_vis//3, y2),
                'center': (w_vis//3, y1, 2*w_vis//3, y2),
                'right': (2*w_vis//3, y1, w_vis, y2),
            }
            
            for name, (x1, y1_b, x2, y2_b) in boxes.items():
                cv2.rectangle(img_with_regions, (x1, y1_b), (x2, y2_b), colors[name], 2)
            
            canvas[20:380, 20:660] = img_with_regions
            cv2.putText(canvas, 'RGB Image + Region Crops', (25, 400),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
            
            # ========== å³ä¸Š: æ·±åº¦åœ– ==========
            depth_vis = cv2.applyColorMap(
                (np.clip(depth_resized / 5.0, 0, 1) * 255).astype(np.uint8),
                cv2.COLORMAP_JET
            )
            canvas[20:380, 680:1260] = cv2.resize(depth_vis, (580, 360))
            cv2.putText(canvas, 'GT Depth Map (colormap)', (685, 400),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
            
            # ========== ä¸‹åŠéƒ¨: æ·±åº¦é æ¸¬æ¯”è¼ƒ ==========
            panel_y = 440
            
            cv2.putText(canvas, 'Depth Regression: Prediction vs Ground Truth', (20, panel_y),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
            cv2.putText(canvas, f'Frame: {i+1}/{len(frame_files)}', (550, panel_y),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)
            
            # ä¸‰å€‹å€åŸŸçš„æ¯”è¼ƒæŸ±ç‹€åœ–
            bar_start_x = 50
            bar_width = 80
            bar_gap = 120
            bar_max_height = 150
            bar_y_base = 650
            
            region_names = ['Left', 'Center', 'Right']
            region_keys = ['left', 'center', 'right']
            
            for j, (name, key) in enumerate(zip(region_names, region_keys)):
                x_center = bar_start_x + j * (bar_width + bar_gap) + bar_width // 2
                
                # GT bar (è—è‰²)
                gt_h = int((gt_depths[key] / 5.0) * bar_max_height)
                gt_h = min(gt_h, bar_max_height)
                cv2.rectangle(canvas, 
                             (x_center - 35, bar_y_base - gt_h),
                             (x_center - 5, bar_y_base),
                             (255, 150, 50), -1)
                
                # Pred bar (ç¶ è‰²)
                pred_h = int((pred_depths[key] / 5.0) * bar_max_height)
                pred_h = min(pred_h, bar_max_height)
                cv2.rectangle(canvas,
                             (x_center + 5, bar_y_base - pred_h),
                             (x_center + 35, bar_y_base),
                             (50, 255, 50), -1)
                
                # å€åŸŸåç¨±
                cv2.putText(canvas, name, (x_center - 25, bar_y_base + 25),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, colors[key], 1)
                
                # æ•¸å€¼æ¨™ç±¤
                cv2.putText(canvas, f'GT:{gt_depths[key]:.2f}m', (x_center - 45, panel_y + 50),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255, 150, 50), 1)
                cv2.putText(canvas, f'Pred:{pred_depths[key]:.2f}m', (x_center - 45, panel_y + 75),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.45, (50, 255, 50), 1)
                
                # èª¤å·®
                error = abs(pred_depths[key] - gt_depths[key])
                err_color = (0, 255, 0) if error < 0.5 else (0, 165, 255) if error < 1.0 else (0, 0, 255)
                cv2.putText(canvas, f'Err:{error:.2f}m', (x_center - 40, panel_y + 100),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.45, err_color, 1)
            
            # åœ–ä¾‹
            legend_x = 450
            legend_y = panel_y + 50
            cv2.rectangle(canvas, (legend_x, legend_y), (legend_x + 20, legend_y + 15), (255, 150, 50), -1)
            cv2.putText(canvas, 'Ground Truth', (legend_x + 30, legend_y + 12),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 150, 50), 1)
            cv2.rectangle(canvas, (legend_x, legend_y + 25), (legend_x + 20, legend_y + 40), (50, 255, 50), -1)
            cv2.putText(canvas, 'Prediction', (legend_x + 30, legend_y + 37),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (50, 255, 50), 1)
            
            # ========== å³ä¸‹: èª¤å·®æ›²ç·š ==========
            graph_x = 650
            graph_y = panel_y + 40
            graph_w = 350
            graph_h = 120
            
            cv2.rectangle(canvas, (graph_x, graph_y), (graph_x + graph_w, graph_y + graph_h),
                         (50, 50, 50), -1)
            
            # ç¹ªè£½èª¤å·®æ›²ç·š
            if len(errors_center) > 1:
                max_err = 2.0  # æœ€å¤§èª¤å·® 2m
                
                # 1m åƒè€ƒç·š
                ref_y = graph_y + graph_h - int(1.0 / max_err * graph_h)
                cv2.line(canvas, (graph_x, ref_y), (graph_x + graph_w, ref_y), (100, 100, 100), 1)
                cv2.putText(canvas, '1m', (graph_x - 25, ref_y + 5),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.35, (100, 100, 100), 1)
                
                # ç¹ªè£½ä¸‰æ¢æ›²ç·š
                for errors, color, label in [
                    (errors_left, (100, 100, 255), 'L'),
                    (errors_center, (100, 255, 100), 'C'),
                    (errors_right, (255, 100, 100), 'R'),
                ]:
                    points = []
                    recent = errors[-50:]  # æœ€è¿‘ 50 å¹€
                    for k, err in enumerate(recent):
                        x = graph_x + int(k * graph_w / max(len(recent) - 1, 1))
                        y = graph_y + graph_h - int(min(err, max_err) / max_err * graph_h)
                        points.append((x, y))
                    
                    if len(points) > 1:
                        for k in range(len(points) - 1):
                            cv2.line(canvas, points[k], points[k+1], color, 1)
                
                cv2.putText(canvas, 'Prediction Error (m) - L/C/R', (graph_x, graph_y - 5),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (200, 200, 200), 1)
            
            # ========== çµ±è¨ˆæ•¸æ“š ==========
            stats_x = 650
            stats_y = graph_y + graph_h + 30
            
            avg_error = (np.mean(errors_left) + np.mean(errors_center) + np.mean(errors_right)) / 3 if errors_center else 0
            cv2.putText(canvas, f'Mean Abs Error: {avg_error:.3f}m', (stats_x, stats_y),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
            
            # è©•åˆ†
            if avg_error < 0.3:
                grade, grade_color = 'Excellent', (0, 255, 0)
            elif avg_error < 0.5:
                grade, grade_color = 'Good', (0, 255, 255)
            elif avg_error < 1.0:
                grade, grade_color = 'Fair', (0, 165, 255)
            else:
                grade, grade_color = 'Poor', (0, 0, 255)
            
            cv2.putText(canvas, f'Grade: {grade}', (stats_x + 250, stats_y),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, grade_color, 2)
            
            # æ¨™é¡Œ
            cv2.putText(canvas, f'TempoVLM: Depth Regression Demo', (20, frame_height - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 100, 100), 1)
            
            out.write(canvas)
        
        out.release()
        
        # è¼¸å‡ºçµ±è¨ˆ
        if errors_center:
            avg_err_all = (np.mean(errors_left) + np.mean(errors_center) + np.mean(errors_right)) / 3
            print(f"  ğŸ“Š å¹³å‡æ·±åº¦é æ¸¬èª¤å·®: {avg_err_all:.3f}m")
            print(f"      Left: {np.mean(errors_left):.3f}m, Center: {np.mean(errors_center):.3f}m, Right: {np.mean(errors_right):.3f}m")
        
        print(f"âœ… å½±ç‰‡å·²ä¿å­˜: {output_path}")
        
        return {
            'total_frames': len(frame_files),
            'mean_error': float(avg_err_all) if errors_center else 0,
            'errors': {
                'left': float(np.mean(errors_left)) if errors_left else 0,
                'center': float(np.mean(errors_center)) if errors_center else 0,
                'right': float(np.mean(errors_right)) if errors_right else 0,
            }
        }
    
    # ========== 3. è»Œè·¡è¦–è¦ºåŒ– (æ”¹é€²ç‰ˆ - å« GT vs Pred å°æ¯”) ==========
    
    def visualize_trajectory(self, scene_dir, output_path, max_frames=60):
        """
        ç”Ÿæˆè»Œè·¡é æ¸¬å½±ç‰‡ - åŒ…å« GT vs Predicted å°æ¯”å’Œèª¤å·®çµ±è¨ˆ
        
        é¡¯ç¤ºå…§å®¹:
        1. åŸå§‹ RGB åœ–åƒ
        2. ä¿¯è¦–è»Œè·¡åœ– (GT ç¶ è‰², Predicted ç´…è‰²)
        3. ä½ç½®èª¤å·® (ATE)
        4. è»Œè·¡çµ±è¨ˆ
        """
        print("\nğŸ¬ ç”Ÿæˆè»Œè·¡é æ¸¬å½±ç‰‡...")
        
        color_dir = scene_dir / 'color'
        pose_dir = scene_dir / 'pose'
        
        frame_files = sorted(color_dir.glob('*.jpg'))[:max_frames]
        
        if len(frame_files) < 10:
            print("âŒ å¹€æ•¸ä¸è¶³")
            return None
        
        # è®€å– GT poses
        gt_positions = []
        for frame_file in frame_files:
            pose_file = pose_dir / (frame_file.stem + '.txt')
            if pose_file.exists():
                try:
                    pose = np.loadtxt(pose_file)
                    if pose.shape == (4, 4):
                        pos = pose[:3, 3]
                        gt_positions.append(pos)
                    else:
                        gt_positions.append(gt_positions[-1] if gt_positions else np.array([0, 0, 0]))
                except:
                    gt_positions.append(gt_positions[-1] if gt_positions else np.array([0, 0, 0]))
            else:
                gt_positions.append(gt_positions[-1] if gt_positions else np.array([0, 0, 0]))
        
        gt_positions = np.array(gt_positions)
        
        # é æ¸¬è»Œè·¡ (ä½¿ç”¨ motion head æˆ–æ¨¡æ“¬)
        print("  é æ¸¬é‹å‹•...")
        
        # åˆå§‹ä½ç½®è¨­ç‚º GT çš„ç¬¬ä¸€å€‹ä½ç½®ï¼ˆç¢ºä¿èµ·é»ä¸€è‡´ï¼‰
        pred_positions = [gt_positions[0].copy()]
        prev_feat = None
        
        self.clear_temporal_buffer()
        # é‡ç½® GRU hidden state
        if hasattr(self, 'gru_hidden_state'):
            self.gru_hidden_state = None
        
        use_motion_head = hasattr(self.unified_model, 'motion_head')
        
        # å„²å­˜é¡å¤–è³‡è¨Š (å“è³ªã€ä¸ç¢ºå®šæ€§ç­‰)
        motion_qualities = []
        motion_uncertainties = []
        
        for i, frame_file in enumerate(tqdm(frame_files, desc="  æå–ç‰¹å¾µ")):
            img = Image.open(frame_file).convert('RGB')
            feat = self.extract_features(img)
            
            if prev_feat is not None and i < len(gt_positions):
                if use_motion_head:
                    with torch.no_grad():
                        # æ–° APIï¼šåŒæ™‚è¿”å›å¤šå€‹è¼¸å‡º
                        outputs, _ = self.unified_model(feat, prev_feat, tasks=['motion'])
                        pred_motion = outputs['motion'].cpu().numpy()[0]
                        pred_positions.append(pred_positions[-1] + pred_motion[:3])
                        
                        # æ”¶é›†é¡å¤–è³‡è¨Šï¼ˆå¦‚æœæœ‰ï¼‰
                        if 'motion_quality' in outputs:
                            motion_qualities.append(outputs['motion_quality'].cpu().item())
                        if 'motion_uncertainty' in outputs:
                            motion_uncertainties.append(outputs['motion_uncertainty'].cpu().numpy()[0])
                else:
                    # æ¨¡æ“¬é æ¸¬ï¼šåŸºæ–¼ GT é‹å‹• + èˆ‡é‹å‹•å¹…åº¦æˆæ¯”ä¾‹çš„å°å™ªè²
                    gt_motion = gt_positions[i] - gt_positions[i-1]
                    motion_scale = np.linalg.norm(gt_motion)
                    noise = np.random.randn(3) * max(0.01, motion_scale * 0.1)
                    pred_positions.append(pred_positions[-1] + gt_motion + noise)
            
            prev_feat = feat
        
        pred_positions = np.array(pred_positions)
        
        # ç¢ºä¿é•·åº¦ä¸€è‡´
        min_len = min(len(gt_positions), len(pred_positions))
        gt_positions_aligned = gt_positions[:min_len]
        pred_positions_aligned = pred_positions[:min_len]
        
        # ä¸­å¿ƒåŒ–è»Œè·¡ï¼ˆä»¥ç¬¬ä¸€å€‹é»ç‚ºåŸé»ï¼Œæ›´ç›´è§€çš„æ¯”è¼ƒï¼‰
        gt_centered = gt_positions_aligned - gt_positions_aligned[0]
        pred_centered = pred_positions_aligned - pred_positions_aligned[0]
        
        # ä¸åšå°ºåº¦å°é½Šï¼Œå› ç‚ºèµ·é»å·²ç¶“ä¸€è‡´
        pred_scaled = pred_centered
        
        # è¨ˆç®— ATE (Absolute Trajectory Error)
        ate_errors = []
        for i in range(len(gt_centered)):
            ate = np.linalg.norm(gt_centered[i] - pred_scaled[i])
            ate_errors.append(ate)
        
        # ç”Ÿæˆå½±ç‰‡
        frame_width = 1280
        frame_height = 720
        fps = 10
        
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(str(output_path), fourcc, fps, (frame_width, frame_height))
        
        for i, frame_file in enumerate(tqdm(frame_files, desc="  å¯«å…¥å¹€")):
            img = cv2.imread(str(frame_file))
            img = cv2.resize(img, (640, 360))
            
            # å‰µå»ºç•«å¸ƒ
            canvas = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)
            canvas[:] = (30, 30, 30)
            
            # æ”¾ç½®ä¸»åœ–
            canvas[20:380, 20:660] = img
            cv2.putText(canvas, 'RGB Image', (25, 400),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
            
            # ========== ç¹ªè£½ä¿¯è¦–åœ– ==========
            traj_size = 350
            traj_x = 700
            traj_y = 20
            
            # èƒŒæ™¯
            cv2.rectangle(canvas, (traj_x, traj_y), (traj_x + traj_size, traj_y + traj_size),
                         (50, 50, 50), -1)
            
            # ç¶²æ ¼
            for j in range(5):
                offset = int(j * traj_size / 4)
                cv2.line(canvas, (traj_x + offset, traj_y), (traj_x + offset, traj_y + traj_size),
                        (70, 70, 70), 1)
                cv2.line(canvas, (traj_x, traj_y + offset), (traj_x + traj_size, traj_y + offset),
                        (70, 70, 70), 1)
            
            # è¨ˆç®—é¡¯ç¤ºç¯„åœ
            all_pos = np.vstack([gt_centered[:i+1], pred_scaled[:min(i+1, len(pred_scaled))]])
            if len(all_pos) > 0:
                x_range = max(abs(all_pos[:, 0].max()), abs(all_pos[:, 0].min()), 1.0)
                z_range = max(abs(all_pos[:, 2].max()), abs(all_pos[:, 2].min()), 1.0)
                scale = min(traj_size / (2.2 * x_range), traj_size / (2.2 * z_range))
            else:
                scale = 50
            
            center_x = traj_x + traj_size // 2
            center_y = traj_y + traj_size // 2
            
            # ç¹ªè£½ GT è»Œè·¡ (ç¶ è‰²)
            gt_points = []
            for j in range(i + 1):
                px = int(center_x + gt_centered[j, 0] * scale)
                py = int(center_y - gt_centered[j, 2] * scale)
                gt_points.append((px, py))
            
            if len(gt_points) > 1:
                for j in range(len(gt_points) - 1):
                    cv2.line(canvas, gt_points[j], gt_points[j+1], (100, 255, 100), 2)
            
            # ç¹ªè£½é æ¸¬è»Œè·¡ (ç´…è‰²)
            if i < len(pred_scaled):
                pred_points = []
                for j in range(min(i + 1, len(pred_scaled))):
                    px = int(center_x + pred_scaled[j, 0] * scale)
                    py = int(center_y - pred_scaled[j, 2] * scale)
                    pred_points.append((px, py))
                
                if len(pred_points) > 1:
                    for j in range(len(pred_points) - 1):
                        cv2.line(canvas, pred_points[j], pred_points[j+1], (100, 100, 255), 2)
            
            # ç•¶å‰ä½ç½®æ¨™è¨˜
            if gt_points:
                cv2.circle(canvas, gt_points[-1], 8, (100, 255, 100), -1)
            if i < len(pred_scaled) and pred_points:
                cv2.circle(canvas, pred_points[-1], 8, (100, 100, 255), -1)
            
            # èµ·é»æ¨™è¨˜
            if gt_points:
                cv2.circle(canvas, gt_points[0], 5, (255, 255, 255), -1)
                cv2.putText(canvas, 'Start', (gt_points[0][0] + 10, gt_points[0][1]),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
            
            # è»Œè·¡æ¨™é¡Œ
            cv2.putText(canvas, 'Top-down Trajectory View', (traj_x, traj_y + traj_size + 25),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
            
            # åœ–ä¾‹
            legend_y = traj_y + traj_size + 50
            cv2.line(canvas, (traj_x, legend_y), (traj_x + 30, legend_y), (100, 255, 100), 2)
            cv2.putText(canvas, 'GT Trajectory', (traj_x + 40, legend_y + 5),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 255, 100), 1)
            
            cv2.line(canvas, (traj_x, legend_y + 25), (traj_x + 30, legend_y + 25), (100, 100, 255), 2)
            cv2.putText(canvas, 'Predicted', (traj_x + 40, legend_y + 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 100, 255), 1)
            
            # ========== èª¤å·®çµ±è¨ˆé¢æ¿ ==========
            stats_x = 20
            stats_y = 430
            
            cv2.putText(canvas, 'Trajectory Prediction: GT vs Predicted', (stats_x, stats_y),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
            cv2.putText(canvas, f'Frame: {i+1}/{len(frame_files)}', (stats_x + 450, stats_y),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)
            
            # ç•¶å‰ä½ç½® (é¡¯ç¤ºç›¸å°æ–¼èµ·é»çš„ä½ç½®)
            if i < len(gt_centered):
                pos = gt_centered[i]
                cv2.putText(canvas, f'GT Position: ({pos[0]:.2f}, {pos[1]:.2f}, {pos[2]:.2f}) m', 
                           (stats_x, stats_y + 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 255, 100), 1)
            
            if i < len(pred_scaled):
                pred_pos = pred_scaled[i]
                cv2.putText(canvas, f'Pred Position: ({pred_pos[0]:.2f}, {pred_pos[1]:.2f}, {pred_pos[2]:.2f}) m', 
                           (stats_x, stats_y + 65), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 100, 255), 1)
            
            # ATE èª¤å·®
            if i > 0 and i < len(ate_errors):
                current_ate = ate_errors[i]
                mean_ate = np.mean(ate_errors[:i+1])
                
                ate_color = (0, 255, 0) if current_ate < 0.3 else (0, 165, 255) if current_ate < 0.5 else (0, 0, 255)
                cv2.putText(canvas, f'Current ATE: {current_ate:.3f}m', (stats_x, stats_y + 100),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, ate_color, 2)
                
                cv2.putText(canvas, f'Mean ATE: {mean_ate:.3f}m', (stats_x + 250, stats_y + 100),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
            
            # ========== èª¤å·®æ›²ç·š ==========
            graph_x = 20
            graph_y = stats_y + 130
            graph_w = 400
            graph_h = 100
            
            cv2.rectangle(canvas, (graph_x, graph_y), (graph_x + graph_w, graph_y + graph_h),
                         (50, 50, 50), -1)
            
            if i > 1 and len(ate_errors) > 1:
                max_err = max(max(ate_errors[:i+1]), 1.0)
                
                # 0.5m åƒè€ƒç·š
                ref_y = graph_y + graph_h - int(0.5 / max_err * graph_h)
                cv2.line(canvas, (graph_x, ref_y), (graph_x + graph_w, ref_y), (100, 100, 100), 1)
                cv2.putText(canvas, '0.5m', (graph_x - 35, ref_y + 5),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.35, (100, 100, 100), 1)
                
                # ç¹ªè£½èª¤å·®æ›²ç·š
                points = []
                recent = ate_errors[:i+1][-50:]  # æœ€è¿‘ 50 å¹€
                for k, err in enumerate(recent):
                    x = graph_x + int(k * graph_w / max(len(recent) - 1, 1))
                    y = graph_y + graph_h - int(min(err, max_err) / max_err * graph_h)
                    points.append((x, y))
                
                if len(points) > 1:
                    for k in range(len(points) - 1):
                        color = (0, 255, 0) if recent[k] < 0.5 else (0, 0, 255)
                        cv2.line(canvas, points[k], points[k+1], color, 2)
                
                cv2.putText(canvas, 'ATE Error Over Time (green < 0.5m)', (graph_x, graph_y - 5),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (200, 200, 200), 1)
            
            # ========== è»Œè·¡é•·åº¦çµ±è¨ˆ ==========
            if i > 0:
                gt_length = np.sum(np.linalg.norm(np.diff(gt_centered[:i+1], axis=0), axis=1))
                pred_length = np.sum(np.linalg.norm(np.diff(pred_scaled[:min(i+1, len(pred_scaled))], axis=0), axis=1)) if i < len(pred_scaled) else 0
                
                cv2.putText(canvas, f'GT Path Length: {gt_length:.2f}m', (graph_x + 450, graph_y + 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 255, 100), 1)
                cv2.putText(canvas, f'Pred Path Length: {pred_length:.2f}m', (graph_x + 450, graph_y + 55),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 100, 255), 1)
            
            # æ¨™é¡Œ
            cv2.putText(canvas, f'TempoVLM: Motion Prediction Demo - Frame {i+1}/{len(frame_files)}',
                       (20, frame_height - 20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 100, 100), 1)
            
            out.write(canvas)
        
        out.release()
        
        # è¼¸å‡ºçµ±è¨ˆ
        if ate_errors:
            final_ate = np.mean(ate_errors)
            print(f"  ğŸ“Š æœ€çµ‚å¹³å‡ ATE: {final_ate:.3f}m")
        
        print(f"âœ… å½±ç‰‡å·²ä¿å­˜: {output_path}")
        
        return {
            'total_frames': len(frame_files),
            'trajectory_length': float(np.sum(np.linalg.norm(np.diff(gt_positions, axis=0), axis=1))),
            'mean_ate': float(np.mean(ate_errors)) if ate_errors else 0
        }
    
    # ========== 4. é®æ“‹æ¸¬è©¦è¦–è¦ºåŒ– (NEW) ==========
    
    def visualize_occlusion_test(self, scene_dir, output_path, max_frames=40,
                                  occlusion_start=5, occlusion_gap=5,
                                  occlusion_ratio=0.4, occlusion_type='black',
                                  occlusion_frames=None,
                                  injection_method='full', anomaly_threshold=0.25,
                                  segment_length=3):
        """
        ç”Ÿæˆé®æ“‹æ¸¬è©¦è¦–è¦ºåŒ–å½±ç‰‡ - ä»‹é¢é¢¨æ ¼ä»¿ç…§åŸç‰ˆ visualization_demo.py
        
        Args:
            scene_dir: å ´æ™¯ç›®éŒ„
            output_path: è¼¸å‡ºè·¯å¾‘
            max_frames: æœ€å¤§å¹€æ•¸
            occlusion_start: é–‹å§‹é®æ“‹çš„å¹€æ•¸ï¼ˆé è¨­ç¬¬ 5 å¹€ï¼‰
            occlusion_gap: å€é–“é–“éš”ï¼ˆå¹€æ•¸ï¼‰ï¼Œé è¨­ 5
            occlusion_ratio: é®æ“‹å€åŸŸæ¯”ä¾‹ï¼ˆç”¨æ–¼ YOLO å¤±æ•—æ™‚çš„å‚™ç”¨é®æ“‹ï¼‰
            occlusion_type: é®æ“‹é¡å‹
            injection_method: æ³¨å…¥æ–¹æ³•
            anomaly_threshold: ç•°å¸¸æª¢æ¸¬é–¾å€¼ (é è¨­ 0.25)
            segment_length: æ¯å€‹é®æ“‹å€é–“é•·åº¦ï¼ˆå¹€æ•¸ï¼‰ï¼Œé è¨­ 3
        """
        print("\nğŸ¬ ç”Ÿæˆé®æ“‹æ¸¬è©¦è¦–è¦ºåŒ–...")
        
        color_dir = scene_dir / 'color'
        frame_files = sorted(color_dir.glob('*.jpg'))[:max_frames]
        
        if len(frame_files) < 10:
            print("âŒ å¹€æ•¸ä¸è¶³")
            return None
        
        self.clear_temporal_buffer()
        
        # åˆå§‹åŒ–è¨˜æ†¶ç·©è¡å€
        memory_buffer = AdaptiveMemoryBuffer(max_size=8, anomaly_threshold=anomaly_threshold)

        # é€£çºŒé®æ“‹æ¨¡å¼çš„çµæŸå¹€ï¼ˆç”¨æ–¼é¡¯ç¤ºèˆ‡å¾Œå‚™æ¨¡å¼ï¼Œé¿å…æœªå®šç¾©ï¼‰
        occlusion_end = min(len(frame_files), occlusion_start + segment_length)
        
        # åˆå§‹åŒ– YOLO é®æ“‹å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        yolo_occluder = None
        if occlusion_type.startswith('yolo_') and YOLO_AVAILABLE:
            print("ğŸ“¦ åˆå§‹åŒ– YOLO ç‰©ä»¶åµæ¸¬å™¨...")
            # é™ä½ä¿¡å¿ƒåº¦é–¾å€¼ä»¥åµæ¸¬æ›´å¤šç‰©ä»¶
            yolo_occluder = YOLOOccluder(model_size='n', confidence_threshold=0.15)
            print("âœ… YOLO å·²å°±ç·’ (confidence=0.15, æ›´æ•æ„Ÿ)")
        elif occlusion_type.startswith('yolo_') and not YOLO_AVAILABLE:
            print("âš ï¸ YOLO ä¸å¯ç”¨ï¼Œæ”¹ç”¨ black é®æ“‹")
            occlusion_type = 'black'
        
        frame_width = 1280
        frame_height = 720
        fps = 5  # è¼ƒæ…¢çš„ fps ä»¥ä¾¿è§€çœ‹æ–‡å­—
        
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(str(output_path), fourcc, fps, (frame_width, frame_height))
        
        results = []
        
        # ç´¯ç©çµ±è¨ˆ
        total_occluded = 0
        total_detected = 0
        total_injected = 0
        detection_history = []
        
        # è§£æé®æ“‹å¹€åˆ—è¡¨ (å¦‚æœæœ‰çš„è©±)
        occlusion_frame_list = []
        if occlusion_frames:
            if isinstance(occlusion_frames, str):
                occlusion_frame_list = [int(x.strip()) for x in occlusion_frames.split(',')]
            elif isinstance(occlusion_frames, list):
                occlusion_frame_list = occlusion_frames
        else:
            # ç”Ÿæˆå¤šå€‹å°å€é–“çš„é®æ“‹å¹€åˆ—è¡¨
            # occlusion_start: é–‹å§‹é®æ“‹çš„å¹€æ•¸
            # occlusion_gap: å€é–“é–“éš”
            
            occlusion_frame_list = []
            current_pos = occlusion_start  # å¾æŒ‡å®šå¹€é–‹å§‹
            seg_count = 0
            
            # æŒçºŒç”Ÿæˆå€é–“ç›´åˆ°å½±ç‰‡çµæŸ
            while current_pos + segment_length <= len(frame_files):
                # æ·»åŠ é€™å€‹å€é–“çš„æ‰€æœ‰å¹€
                for offset in range(segment_length):
                    occlusion_frame_list.append(current_pos + offset)
                seg_count += 1
                # ç§»å‹•åˆ°ä¸‹ä¸€å€‹å€é–“ï¼ˆå€é–“é•·åº¦ + å›ºå®šé–“éš”ï¼‰
                current_pos += segment_length + occlusion_gap
            
            print(f"  ğŸ¯ ç”Ÿæˆ {seg_count} å€‹å°å€é–“é®æ“‹ (å¾ç¬¬ {occlusion_start} å¹€é–‹å§‹):")
            print(f"     - æ¯å€‹å€é–“é•·åº¦: {segment_length} å¹€")
            print(f"     - å€é–“é–“éš”: {occlusion_gap} å¹€ (å›ºå®š)")
            print(f"     - ç¸½é®æ“‹å¹€æ•¸: {len(occlusion_frame_list)}")
            print(f"     - å½±ç‰‡ç¸½å¹€æ•¸: {len(frame_files)}")
            if len(occlusion_frame_list) <= 20:
                print(f"     - é®æ“‹å¹€: {occlusion_frame_list}")
        
        for i, frame_file in enumerate(tqdm(frame_files, desc="  è™•ç†å¹€")):
            original_img = Image.open(frame_file).convert('RGB')
            original_cv = cv2.cvtColor(np.array(original_img), cv2.COLOR_RGB2BGR)
            original_cv_clean = original_cv.copy()  # ä¿ç•™åŸå§‹åœ–åƒç”¨æ–¼é¡¯ç¤º
            
            # æ˜¯å¦åŠ å…¥é®æ“‹
            if occlusion_frame_list:
                is_occluded = i in occlusion_frame_list
            else:
                is_occluded = occlusion_start <= i < occlusion_end
            occluded_cv = original_cv.copy()
            
            if is_occluded:
                total_occluded += 1
                h, w = occluded_cv.shape[:2]
                cx, cy = w // 2, h // 2
                # è¨ˆç®— 70% é®æ“‹å€åŸŸå¤§å°ï¼ˆç”¨æ–¼ YOLO å¤±æ•—æ™‚ï¼‰
                fallback_size = int(min(w, h) * 0.70 / 2)  # 70% çš„åŠå¾‘
                # åŸå§‹é®æ“‹å¤§å°ï¼ˆç”¨æ–¼å…¶ä»–å‚³çµ±æ–¹å¼ï¼‰
                size = int(min(w, h) * occlusion_ratio / 2)
                
                occluded_object_info = None  # è¨˜éŒ„è¢«é®æ“‹çš„ç‰©ä»¶è³‡è¨Š
                
                # ========== YOLO ç‰©ä»¶é®æ“‹ ==========
                if occlusion_type.startswith('yolo_') and yolo_occluder:
                    # è§£æç›®æ¨™é¡åˆ¥ï¼ˆNone = åµæ¸¬æ‰€æœ‰ç‰©ä»¶ï¼‰
                    if occlusion_type == 'yolo_indoor':
                        target_classes = ['chair', 'couch', 'dining table', 'bed', 'tv']
                    elif occlusion_type == 'yolo_furniture':
                        target_classes = ['chair', 'couch', 'dining table', 'bed']
                    elif occlusion_type == 'yolo_chair':
                        target_classes = ['chair']
                    elif occlusion_type == 'yolo_all':
                        target_classes = None  # åµæ¸¬æ‰€æœ‰ç‰©ä»¶
                    else:
                        target_classes = None  # é è¨­åµæ¸¬æ‰€æœ‰ç‰©ä»¶
                    
                    # å°ç•¶å‰å¹€é€²è¡Œ YOLO åµæ¸¬ä¸¦é®æ“‹æœ€å¤š 3 å€‹ä¸­ç­‰ç‰©ä»¶
                    occluded_cv, selected_objects, all_detections = yolo_occluder.occlude_multiple_objects(
                        occluded_cv,
                        target_classes=target_classes,
                        occlusion_color=(0, 0, 0),  # é»‘è‰²é®æ“‹
                        min_area=1000,               # é™ä½æœ€å°ç‰©ä»¶é¢ç©ï¼ˆåŸæœ¬ 2000 å¤ªåš´æ ¼ï¼‰
                        max_objects=2,              # æœ€å¤šé®æ“‹ 3 å€‹ç‰©ä»¶
                        size_preference='medium'    # åå¥½ä¸­ç­‰å¤§å°çš„ç‰©ä»¶
                    )
                    
                    if selected_objects:
                        # è¨˜éŒ„è¢«é®æ“‹ç‰©ä»¶çš„è³‡è¨Š
                        occluded_object_info = {
                            'count': len(selected_objects),
                            'objects': []
                        }
                        
                        for obj in selected_objects:
                            x1, y1, x2, y2 = obj['bbox']
                            occluded_object_info['objects'].append({
                                'class_name': obj['class_name'],
                                'confidence': float(obj['confidence']),
                                'bbox': [int(x1), int(y1), int(x2), int(y2)],
                                'area': int(obj['area']),
                                'area_ratio': float(obj['area']) / (w * h)
                            })
                        
                        # åœ¨ç¬¬ä¸€å€‹é®æ“‹å¹€æ™‚é¡¯ç¤ºè³‡è¨Š
                        if i == occlusion_start or (occlusion_frame_list and i == min(occlusion_frame_list)):
                            print(f"\n  ğŸ¯ YOLO åµæ¸¬åˆ° {len(all_detections)} å€‹ç‰©ä»¶")
                            print(f"  ğŸš« é®æ“‹äº† {len(selected_objects)} å€‹ç‰©ä»¶:")
                            for obj in selected_objects:
                                print(f"     - {obj['class_name']}: {obj['confidence']:.2f} "
                                      f"(area: {obj['area']}, {obj['area']/(w*h)*100:.1f}%)")
                    else:
                        # æ²’æœ‰åµæ¸¬åˆ°é©åˆçš„ç‰©ä»¶ï¼Œä½¿ç”¨ 70% ä¸­å¤®é»‘è‰²é®æ“‹
                        if i == occlusion_start or (occlusion_frame_list and i == min(occlusion_frame_list) if occlusion_frame_list else True):
                            print(f"\n  âš ï¸ å¹€ {i}: YOLO æ²’æœ‰åµæ¸¬åˆ°ç¬¦åˆæ¢ä»¶çš„ç‰©ä»¶")
                            if all_detections:
                                print(f"     ç¸½å…±åµæ¸¬åˆ° {len(all_detections)} å€‹ç‰©ä»¶ï¼Œä½†éƒ½å¤ªå° (< 2000 px)")
                            print(f"     æ”¹ç”¨ä¸­å¤®é»‘è‰²é®æ“‹ (70% è¦†è“‹)")
                        cv2.rectangle(occluded_cv, (cx-fallback_size, cy-fallback_size), 
                                    (cx+fallback_size, cy+fallback_size), (0, 0, 0), -1)
                
                input_img = Image.fromarray(cv2.cvtColor(occluded_cv, cv2.COLOR_BGR2RGB))
            else:
                input_img = original_img
                occluded_object_info = None
            
            # æå–ç‰¹å¾µ
            # æå–ç‰¹å¾µ
            feat = self.extract_features(input_img)
            adapter_meta = getattr(self, 'last_adapter_meta', None)
            
            # æå–é‚Šç·£ç‰¹å¾µ (v6.1 Logic) - ç”¨æ–¼å ´æ™¯åŒ¹é…
            # å¿…é ˆå°æ¯ä¸€å¹€éƒ½æå–ï¼Œé€™æ¨£è¨˜æ†¶åº«è£¡æ‰æœƒæœ‰
            edge_feat_to_store = self.extract_edge_features(input_img)
            
            # åŠ å…¥è¨˜æ†¶åº«
            result = memory_buffer.add_frame(feat, i, input_img, adapter_meta=adapter_meta, edge_feat=edge_feat_to_store)
            if len(result) == 5:
                added, quality, anomaly_score, is_anomaly, debug_info = result
            else:
                added, quality, anomaly_score, is_anomaly = result
                debug_info = {'image_occlusion': 0.0}
            
            img_occ = debug_info.get('image_occlusion', 0.0)
            
            if is_occluded and is_anomaly:
                total_detected += 1
            
            # æ›´æ–°æª¢æ¸¬æ­·å²
            if total_occluded > 0:
                detection_history.append(total_detected / total_occluded)
            
            # å¦‚æœç•°å¸¸ï¼Œå˜—è©¦æ³¨å…¥
            injection_result = None
            gt_response = ""
            occluded_response = ""
            injected_response = ""
            
            # æå–é‚Šç·£ç‰¹å¾µ (v6.1 Logic)
            if len(memory_buffer.features) > 0:
                edge_feat = self.extract_edge_features(input_img)
            else:
                edge_feat = None

            if is_anomaly and len(memory_buffer.features) > 0:
                best_memory, score, info = memory_buffer.get_best_memory(feat, i, edge_feat=edge_feat)
                
                if best_memory is not None:
                    scene_match = info.get('scene_match', 1.0)
                    adapter_reliability = 1.0
                    if info.get('adapter_meta'):
                        mq = info['adapter_meta'].get('memory_quality')
                        if mq is not None:
                            adapter_reliability = max(0.0, min(1.0, float(mq)))
                    
                    base_strength = memory_buffer.compute_injection_strength(
                        anomaly_score, score,
                        image_occlusion=img_occ,
                        scene_match=scene_match,
                        memory_reliability=adapter_reliability
                    )
                    
                    # ğŸ”¥ æé«˜æ³¨å…¥å¼·åº¦ä¸Šé™ï¼ˆå‹•æ…‹é®æ“‹é®ç½©è®“æ³¨å…¥æ›´ç²¾ç¢ºï¼Œå¯ä»¥æ›´æ¿€é€²ï¼‰
                    if injection_method == 'full':
                        strength = min(0.50, base_strength * 1.0)  # å¾ 0.35 æé«˜åˆ° 0.50
                    elif injection_method == 'adaptive':
                        strength = min(0.55, base_strength * 1.1)  # adaptive æ›´é«˜
                    else:
                        strength = min(0.45, base_strength * 0.95)
                    
                    # ğŸ¯ Prompt ç­–ç•¥å„ªåŒ–ï¼š
                    # 1. GT: æ¨™æº–æè¿°
                    # 2. Occluded: æ¨™æº–æè¿°ï¼ˆæ¸¬è©¦ç´”è¦–è¦º - æ‡‰è©²å¤±æ•—ï¼‰
                    # 3. Injected: å¼·åŒ–è¨˜æ†¶å°å‘ promptï¼ˆæ˜ç¢ºæç¤ºé®æ“‹å’Œæ¢å¾©ï¼‰
                    standard_prompt = "Describe what you see in this image."
                    
                    if is_occluded and occluded_object_info and 'objects' in occluded_object_info:
                        # æœ‰ YOLO ç‰©ä»¶è³‡è¨Šï¼šç”Ÿæˆå…·é«”çš„å¼•å° prompt
                        occluded_classes = [obj['class_name'] for obj in occluded_object_info['objects']]
                        occluded_classes_str = ', '.join(set(occluded_classes))
                        
                        memory_guided_prompt = (
                            f"Some objects in this image are blocked by black occlusion masks. "
                            f"Based on your visual memory and the context of the scene, describe what objects "
                            f"are present in the blocked areas. Pay special attention to any {occluded_classes_str} "
                            f"or similar objects that should be there. Describe the complete scene including the occluded parts."
                        )
                    elif is_occluded:
                        # ç„¡ YOLO è³‡è¨Šï¼šé€šç”¨é®æ“‹æ¢å¾© prompt
                        memory_guided_prompt = (
                            "Parts of this image are covered by black occlusion. "
                            "Based on your visual memory from previous frames and the surrounding context, "
                            "describe what objects or items are likely present in the occluded regions. "
                            "Focus on completing the scene description even for blocked areas."
                        )
                    else:
                        # ç„¡é®æ“‹ï¼šä½¿ç”¨æ¨™æº– prompt
                        memory_guided_prompt = standard_prompt
                    
                    try:
                        # GT æè¿° (åŸåœ– + æ¨™æº– prompt)
                        gt_response = self.generate_description(original_img, standard_prompt)
                        
                        # é®æ“‹åœ–æè¿° (é®æ“‹åœ– + æ¨™æº– promptï¼Œæ¸¬è©¦ç´”è¦–è¦ºèƒ½åŠ› - æ‡‰è©²çœ‹ä¸åˆ°)
                        occluded_response = self.generate_description(input_img, standard_prompt)
                        
                        # æ³¨å…¥å¾Œæè¿° (é®æ“‹åœ– + è¨˜æ†¶æ³¨å…¥ + å¼·åŒ–è¨˜æ†¶å°å‘ prompt + é®æ“‹è³‡è¨Š)
                        injected_response = self.generate_with_injection(
                            input_img, best_memory, memory_guided_prompt, strength, injection_method,
                            occlusion_info=occluded_object_info  # å‚³éé®æ“‹ç‰©ä»¶è³‡è¨Š
                        )
                        
                        injection_result = {
                            'strength': strength,
                            'memory_frame': info['timestamp'],
                            'memory_score': score,
                            'scene_match': scene_match,
                            'memory_quality': adapter_reliability
                        }
                        total_injected += 1
                    except Exception as e:
                        injection_result = {'error': str(e)}
            
            results.append({
                'frame': i,
                'quality': quality,
                'anomaly_score': anomaly_score,
                'image_occlusion': img_occ,
                'is_anomaly': is_anomaly,
                'is_occluded': is_occluded,
                'occluded_object': occluded_object_info,  # æ–°å¢ï¼šè¨˜éŒ„è¢«é®æ“‹çš„ç‰©ä»¶
                'injection': injection_result,
                'gt_response': gt_response,
                'occluded_response': occluded_response,
                'injected_response': injected_response
            })
            
            # ========== å‰µå»ºç•«å¸ƒ (ä»¿ç…§åŸç‰ˆé¢¨æ ¼) ==========
            canvas = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)
            canvas[:] = (30, 30, 30)
            
            # ========== ä¸ŠåŠéƒ¨: å·¦åŸåœ– + å³é®æ“‹åœ– ==========
            # å·¦ä¸Š: åŸå§‹åœ–åƒ (Ground Truth)
            gt_display = cv2.resize(original_cv_clean, (320, 240))
            canvas[20:260, 20:340] = gt_display
            cv2.putText(canvas, 'GT (Original)', (20, 280),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (100, 255, 100), 1)
            
            # å³ä¸Š: è™•ç†å¾Œåœ–åƒ (å¯èƒ½æœ‰é®æ“‹)
            current_display = cv2.resize(occluded_cv if is_occluded else original_cv, (320, 240))
            canvas[20:260, 360:680] = current_display
            label = 'Occluded Input' if is_occluded else 'Input (No Occlusion)'
            label_color = (0, 0, 255) if is_occluded else (200, 200, 200)
            cv2.putText(canvas, label, (360, 280),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, label_color, 1)
            
            # å¦‚æœæœ‰ YOLO é®æ“‹ç‰©ä»¶è³‡è¨Šï¼Œé¡¯ç¤ºåœ¨åœ–åƒä¸‹æ–¹
            if occluded_object_info:
                if 'count' in occluded_object_info:
                    # å¤šç‰©ä»¶é®æ“‹æ ¼å¼
                    obj_names = [obj['class_name'] for obj in occluded_object_info['objects']]
                    total_area_ratio = sum(obj['area_ratio'] for obj in occluded_object_info['objects'])
                    if len(obj_names) == 1:
                        obj_text = f"Occluded: {obj_names[0]} ({total_area_ratio*100:.1f}%)"
                    else:
                        obj_text = f"Occluded: {', '.join(obj_names[:2])}"
                        if len(obj_names) > 2:
                            obj_text += f", +{len(obj_names)-2}"
                        obj_text += f" ({total_area_ratio*100:.1f}%)"
                else:
                    # å–®ç‰©ä»¶é®æ“‹æ ¼å¼ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
                    obj_text = f"Object: {occluded_object_info['class_name']} ({occluded_object_info['area_ratio']*100:.1f}%)"
                cv2.putText(canvas, obj_text, (360, 300),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255, 165, 0), 1)
            
            # ========== å³å´: æŒ‡æ¨™é¢æ¿ (è¡¨æ ¼å¼) ==========
            panel_x = 700
            panel_y = 20
            
            cv2.putText(canvas, 'Occlusion Detection Test', (panel_x, panel_y + 20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
            cv2.putText(canvas, f'Frame: {i+1}/{len(frame_files)}', (panel_x, panel_y + 55),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)
            
            # è¡¨æ ¼æ¨™é¡Œ
            table_y = panel_y + 90
            headers = ['Metric', 'Value', 'Status']
            col_x = [panel_x, panel_x + 150, panel_x + 280]
            
            for j, hdr in enumerate(headers):
                cv2.putText(canvas, hdr, (col_x[j], table_y),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (150, 150, 150), 1)
            
            # è¡¨æ ¼å…§å®¹
            row_data = [
                ('Quality', f'{quality:.3f}', 'NORMAL' if quality > 0.5 else 'LOW'),
                ('Anomaly Score', f'{anomaly_score:.3f}', 'ANOMALY' if is_anomaly else 'OK'),
                ('Image Occ.', f'{img_occ:.3f}', 'BLOCKED' if img_occ > 0.3 else 'CLEAR'),
                ('Memory Size', f'{memory_buffer.get_status()["size"]}', '-'),
            ]
            
            for j, (metric, value, status) in enumerate(row_data):
                row_y = table_y + 30 + j * 28
                
                cv2.putText(canvas, metric, (col_x[0], row_y),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                cv2.putText(canvas, value, (col_x[1], row_y),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
                
                # ç‹€æ…‹é¡è‰²
                if 'ANOMALY' in status or 'BLOCKED' in status:
                    status_color = (0, 0, 255)  # ç´…è‰²
                elif 'LOW' in status:
                    status_color = (0, 165, 255)  # æ©™è‰²
                elif 'OK' in status or 'NORMAL' in status or 'CLEAR' in status:
                    status_color = (0, 255, 0)  # ç¶ è‰²
                else:
                    status_color = (150, 150, 150)
                
                cv2.putText(canvas, status, (col_x[2], row_y),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, status_color, 1)
            
            # ========== ç´¯ç©çµ±è¨ˆ (ä»¿ç…§åŸç‰ˆæº–ç¢ºç‡é¡¯ç¤º) ==========
            stats_x = panel_x
            stats_y = table_y + 150
            
            cv2.putText(canvas, 'Cumulative Stats:', (stats_x, stats_y),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
            
            if total_occluded > 0:
                detection_rate = total_detected / total_occluded
                rate_color = (0, 255, 0) if detection_rate > 0.8 else (0, 165, 255) if detection_rate > 0.5 else (0, 0, 255)
                cv2.putText(canvas, f'Detection Rate: {detection_rate:.1%}', (stats_x, stats_y + 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, rate_color, 2)
                cv2.putText(canvas, f'({total_detected}/{total_occluded})', (stats_x + 200, stats_y + 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
            
            cv2.putText(canvas, f'Injections: {total_injected}', (stats_x, stats_y + 60),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 200, 100), 1)
            
            # ========== ä¸‹åŠéƒ¨: æè¿°å°æ¯”è¡¨æ ¼ ==========
            desc_y = 320
            
            cv2.putText(canvas, 'Response Comparison (GT vs Occluded vs Injected)', (20, desc_y),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            # è¡¨æ ¼æ¨™é¡Œ
            desc_headers = ['Source', 'Response']
            desc_col_x = [20, 180]
            
            cv2.putText(canvas, desc_headers[0], (desc_col_x[0], desc_y + 35),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (150, 150, 150), 1)
            cv2.putText(canvas, desc_headers[1], (desc_col_x[1], desc_y + 35),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (150, 150, 150), 1)
            
            # ä¸‰è¡Œæ¯”è¼ƒ
            if injection_result and 'strength' in injection_result:
                # GT æè¿°
                cv2.putText(canvas, 'GT (Original)', (desc_col_x[0], desc_y + 65),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 255, 100), 1)
                gt_short = gt_response[:90] + '...' if len(gt_response) > 90 else gt_response
                cv2.putText(canvas, gt_short, (desc_col_x[1], desc_y + 65),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (200, 200, 200), 1)
                
                # é®æ“‹æè¿°
                cv2.putText(canvas, 'Occluded', (desc_col_x[0], desc_y + 105),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 100, 255), 1)
                
                # ç°¡å–®æ–·è¡Œ (æ¯ 90 å­—ç¬¦)
                occ_lines = [occluded_response[i:i+90] for i in range(0, len(occluded_response), 90)]
                for k, line in enumerate(occ_lines[:2]): # æœ€å¤šé¡¯ç¤º2è¡Œ
                    cv2.putText(canvas, line + ('...' if k==1 and len(occ_lines)>2 else ''), 
                               (desc_col_x[1], desc_y + 105 + k*20),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.4, (200, 200, 200), 1)
                
                # æ³¨å…¥å¾Œæè¿°
                cv2.putText(canvas, f'Injected (s={injection_result["strength"]:.2f})', (desc_col_x[0], desc_y + 155),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 200, 100), 1)
                
                inj_lines = [injected_response[i:i+90] for i in range(0, len(injected_response), 90)]
                for k, line in enumerate(inj_lines[:2]):
                    cv2.putText(canvas, line + ('...' if k==1 and len(inj_lines)>2 else ''), 
                               (desc_col_x[1], desc_y + 155 + k*20),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.4, (200, 200, 200), 1)
                
                # æ³¨å…¥è³‡è¨Š
                cv2.putText(canvas, f'Memory from Frame {injection_result["memory_frame"]}, score={injection_result["memory_score"]:.2f}',
                           (20, desc_y + 180), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (150, 150, 150), 1)
            else:
                # ç„¡æ³¨å…¥æ™‚çš„æç¤º
                if is_occluded:
                    cv2.putText(canvas, '[No injection triggered - anomaly not detected]', 
                               (desc_col_x[1], desc_y + 85),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 165, 255), 1)
                else:
                    cv2.putText(canvas, '[No occlusion in this frame]', 
                               (desc_col_x[1], desc_y + 85),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (150, 150, 150), 1)
            
            # ========== æª¢æ¸¬ç‡æ›²ç·š (ä»¿ç…§åŸç‰ˆæº–ç¢ºç‡æ›²ç·š) ==========
            if len(detection_history) > 1:
                graph_x = 700
                graph_y = desc_y + 110 # å¾€ä¸‹ç§»ä¸€é»é¿é–‹å¤šè¡Œæ–‡å­—
                graph_w = 350
                graph_h = 80
                
                cv2.rectangle(canvas, (graph_x, graph_y), (graph_x + graph_w, graph_y + graph_h),
                             (50, 50, 50), -1)
                
                # 50% åŸºæº–ç·š
                baseline_y = graph_y + graph_h // 2
                cv2.line(canvas, (graph_x, baseline_y), (graph_x + graph_w, baseline_y),
                        (100, 100, 100), 1)
                cv2.putText(canvas, '50%', (graph_x - 35, baseline_y + 5),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.35, (100, 100, 100), 1)
                
                # ç¹ªè£½æ›²ç·š
                points = []
                max_points = min(len(detection_history), 50)
                step = max(1, len(detection_history) // max_points)
                sampled = detection_history[::step]
                
                for j, rate in enumerate(sampled):
                    # FIX: ä½¿ç”¨ç¸½å¹€æ•¸ len(frame_files) ä½œç‚ºåˆ†æ¯ï¼Œé˜²æ­¢åœ–å½¢æ“ å£“
                    x = graph_x + int(j * step * graph_w / max(len(frame_files) - 1, 1))
                    y = graph_y + graph_h - int(rate * graph_h)
                    points.append((x, y))
                
                if len(points) > 1:
                    for j in range(len(points) - 1):
                        color = (0, 255, 0) if sampled[j] > 0.5 else (0, 0, 255)
                        cv2.line(canvas, points[j], points[j+1], color, 2)
                
                cv2.putText(canvas, 'Detection Rate History (green=above 50%)', (graph_x, graph_y - 5),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (200, 200, 200), 1)
            
            # ========== é€²åº¦æ¢ (ä»¿ç…§åŸç‰ˆ) ==========
            progress_y = 660
            progress_w = 500
            progress = int((i + 1) / len(frame_files) * progress_w)
            
            cv2.rectangle(canvas, (20, progress_y), (20 + progress_w, progress_y + 20), (50, 50, 50), -1)
            cv2.rectangle(canvas, (20, progress_y), (20 + progress, progress_y + 20), (100, 200, 100), -1)
            
            # é®æ“‹å€é–“æ¨™è¨˜
            if occlusion_frame_list:
                # é–ƒçˆæ¨¡å¼ï¼šç•«å¤šå€‹å°æ¨™è¨˜
                for occ_frame in occlusion_frame_list:
                    occ_x = int(20 + occ_frame / len(frame_files) * progress_w)
                    cv2.line(canvas, (occ_x, progress_y - 5), (occ_x, progress_y + 25), (100, 100, 255), 2)
                
                # åªåœ¨ç¬¬ä¸€å€‹æ¨™è¨˜è™•å¯«æ–‡å­—ï¼Œé¿å…é‡ç–Š
                if occlusion_frame_list:
                    first_occ_x = int(20 + occlusion_frame_list[0] / len(frame_files) * progress_w)
                    cv2.putText(canvas, 'Flicker', (first_occ_x, progress_y - 10),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.35, (100, 100, 255), 1)
            else:
                # é€£çºŒæ¨¡å¼ï¼šç•«ä¸€å€‹å¤§æ¡†
                occ_start_x = int(20 + occlusion_start / len(frame_files) * progress_w)
                occ_end_x = int(20 + occlusion_end / len(frame_files) * progress_w)
                cv2.rectangle(canvas, (occ_start_x, progress_y - 5), (occ_end_x, progress_y + 25), (100, 100, 255), 2)
                cv2.putText(canvas, 'Occlusion Zone', (occ_start_x, progress_y - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.35, (100, 100, 255), 1)
            
            # ========== åº•éƒ¨æ¨™é¡Œ ==========
            cv2.putText(canvas, f'TempoVLM: Occlusion Detection & Memory Injection Demo - Frame {i+1}/{len(frame_files)}',
                       (20, frame_height - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 100, 100), 1)
            
            # é…ç½®èªªæ˜
            if occlusion_frame_list:
                 config_text = f'Occlusion: {occlusion_type} (Flickering), {occlusion_ratio:.0%} area'
            else:
                 config_text = f'Occlusion: {occlusion_type}, {occlusion_ratio:.0%} area, frames {occlusion_start}-{occlusion_end}'
            cv2.putText(canvas, config_text, (600, frame_height - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (150, 150, 150), 1)
            
            out.write(canvas)
        
        out.release()
        print(f"âœ… å½±ç‰‡å·²ä¿å­˜: {output_path}")
        
        # è¨ˆç®—çµ±è¨ˆ
        occluded_frames = [r for r in results if r['is_occluded']]
        detected = [r for r in occluded_frames if r['is_anomaly']]
        injected = [r for r in results if r['injection'] and 'strength' in r.get('injection', {})]
        
        stats = {
            'total_frames': len(results),
            'occluded_frames': len(occluded_frames),
            'detected_anomalies': len(detected),
            'detection_rate': len(detected) / max(len(occluded_frames), 1),
            'successful_injections': len(injected),
            'occlusion_config': {
                'start': occlusion_start,  # é–‹å§‹é®æ“‹çš„å¹€æ•¸
                'gap': occlusion_gap,  # å€é–“é–“éš”
                'segment_length': segment_length,
                'ratio': occlusion_ratio,
                'type': occlusion_type
            },
            'detailed_results': results
        }
        
        return stats
    
    # ========== å®Œæ•´ Demo åŸ·è¡Œ ==========
    
    def run_complete_demo(self, data_root, output_dir, split='test', max_scenes=3,
                          occlusion_start=5, occlusion_gap=5, occlusion_ratio=0.4,
                          occlusion_type='black', occlusion_frames=None, 
                          injection_method='full', anomaly_threshold=0.25,
                          segment_length=3,
                          demos=None):
        """
        åŸ·è¡Œå®Œæ•´ Demo
        
        Args:
            demos: è¦åŸ·è¡Œçš„ demo åˆ—è¡¨ï¼Œå¯é¸ ['temporal', 'depth', 'motion', 'occlusion']
                   å¦‚æœç‚º None å‰‡åŸ·è¡Œå…¨éƒ¨
            occlusion_start: é–‹å§‹é®æ“‹çš„å¹€æ•¸ï¼Œé è¨­ 5
            occlusion_gap: å€é–“é–“éš”ï¼ˆå¹€æ•¸ï¼‰ï¼Œé è¨­ 5
            segment_length: æ¯å€‹é®æ“‹å€é–“é•·åº¦ï¼Œé è¨­ 3
        """
        if demos is None:
            demos = ['temporal', 'depth', 'motion', 'occlusion']
        
        print(f"\nğŸ¯ å°‡åŸ·è¡Œçš„ Demo: {demos}")
        
        data_root = Path(data_root)
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ‰¾å ´æ™¯ - æ”¹é€²é‚è¼¯ï¼Œæ”¯æ´ç›´æ¥æŒ‡å®šå ´æ™¯è³‡æ–™å¤¾
        scene_root = data_root
        
        # å¦‚æœ data_root ç›´æ¥æ˜¯å ´æ™¯è³‡æ–™å¤¾ï¼ˆåŒ…å« color å­ç›®éŒ„ï¼‰
        if (data_root / 'color').exists():
            all_scene_dirs = [data_root]
        else:
            # æª¢æŸ¥æ˜¯å¦æ˜¯ scannet_frames_test ç­‰æ¨™æº–ç›®éŒ„çµæ§‹
            if split == 'test' and (data_root / 'scannet_frames_test').exists():
                scene_root = data_root / 'scannet_frames_test'
            elif split == 'train' and (data_root / 'scannet_frames_25k').exists():
                scene_root = data_root / 'scannet_frames_25k'
            
            all_scene_dirs = [d for d in scene_root.iterdir() if d.is_dir() and (d / 'color').exists()]
        
        if not all_scene_dirs:
            print(f"âŒ æ‰¾ä¸åˆ°å ´æ™¯: {scene_root}")
            return
            print(f"âŒ æ‰¾ä¸åˆ°å ´æ™¯: {scene_root}")
            return
        
        # å„ªå…ˆé¸æ“‡ frame æ•¸è¼ƒå¤šçš„å ´æ™¯
        print(f"\nğŸ“Š åˆ†æå ´æ™¯ frame æ•¸é‡...")
        scene_frame_counts = []
        for scene_dir in all_scene_dirs:
            color_dir = scene_dir / 'color'
            if color_dir.exists():
                frame_count = len(list(color_dir.glob('*.jpg')))
            else:
                frame_count = 0
            scene_frame_counts.append((scene_dir, frame_count))
        
        # æŒ‰ frame æ•¸é‡é™åºæ’åº
        scene_frame_counts.sort(key=lambda x: x[1], reverse=True)
        
        # é¸æ“‡å‰ max_scenes å€‹å ´æ™¯
        scene_dirs = [s[0] for s in scene_frame_counts[:max_scenes]]
        
        print(f"\nğŸ“‚ é¸æ“‡äº† {len(scene_dirs)} å€‹å ´æ™¯ (å„ªå…ˆ frame æ•¸å¤šçš„):")
        for scene_dir, frame_count in scene_frame_counts[:max_scenes]:
            print(f"   - {scene_dir.name}: {frame_count} frames")
        
        all_stats = {}
        # é€£çºŒé®æ“‹æ¨¡å¼çš„çµæŸå¹€ï¼ˆæ‘˜è¦ç”¨ï¼›è‹¥ä½¿ç”¨ flicker åˆ—è¡¨å‰‡æœƒé¡¯ç¤º framesï¼‰
        occlusion_end = occlusion_start + segment_length
        
        for scene_dir in scene_dirs:
            scene_name = scene_dir.name
            scene_output_dir = output_dir / scene_name
            scene_output_dir.mkdir(parents=True, exist_ok=True)
            
            print(f"\n{'='*70}")
            print(f"ğŸ¬ è™•ç†å ´æ™¯: {scene_name}")
            print(f"{'='*70}")
            
            scene_stats = {}
            
            try:
                # 1. æ™‚åºä¸€è‡´æ€§
                if 'temporal' in demos:
                    self.clear_temporal_buffer()
                    stats = self.visualize_temporal_consistency(
                        scene_dir,
                        scene_output_dir / 'temporal_consistency.mp4'
                    )
                    if stats:
                        scene_stats['temporal_consistency'] = stats
                
                # 2. æ·±åº¦æ’åº
                if 'depth' in demos:
                    stats = self.visualize_depth_ordering(
                        scene_dir,
                        scene_output_dir / 'depth_ordering.mp4'
                    )
                    if stats:
                        scene_stats['depth_ordering'] = stats
                    
                    # 2.5 æ·±åº¦å›æ­¸
                    stats = self.visualize_depth_regression(
                        scene_dir,
                        scene_output_dir / 'depth_regression.mp4'
                    )
                    if stats:
                        scene_stats['depth_regression'] = stats
                
                # 3. è»Œè·¡
                if 'motion' in demos:
                    self.clear_temporal_buffer()
                    stats = self.visualize_trajectory(
                        scene_dir,
                        scene_output_dir / 'trajectory.mp4'
                    )
                    if stats:
                        scene_stats['trajectory'] = stats
                
                # 4. é®æ“‹æ¸¬è©¦
                if 'occlusion' in demos:
                    self.clear_temporal_buffer()
                    stats = self.visualize_occlusion_test(
                        scene_dir,
                        scene_output_dir / 'occlusion_test.mp4',
                        occlusion_start=occlusion_start,
                        occlusion_gap=occlusion_gap,
                        occlusion_ratio=occlusion_ratio,
                        occlusion_type=occlusion_type,
                        occlusion_frames=occlusion_frames,
                        injection_method=injection_method,
                        anomaly_threshold=anomaly_threshold,
                        segment_length=segment_length
                    )
                    if stats:
                        # ä¿å­˜è©³ç´°çµæœåˆ° JSON
                        occlusion_results_path = scene_output_dir / 'occlusion_results.json'
                        with open(occlusion_results_path, 'w', encoding='utf-8') as f:
                            # ç§»é™¤ detailed_results ä¸­çš„å¤§å‹æ•¸æ“š
                            stats_to_save = {k: v for k, v in stats.items() if k != 'detailed_results'}
                            stats_to_save['frames'] = []
                            for r in stats['detailed_results']:
                                frame_info = {
                                    'frame': r['frame'],
                                    'quality': float(r['quality']),
                                    'anomaly_score': float(r['anomaly_score']),
                                    'image_occlusion': float(r['image_occlusion']),
                                    'is_anomaly': r['is_anomaly'],
                                    'is_occluded': r['is_occluded'],
                                }
                                if r['injection']:
                                    frame_info['injection'] = r['injection']
                                if r['gt_response']:
                                    frame_info['gt_response'] = r['gt_response']
                                if r['occluded_response']:
                                    frame_info['occluded_response'] = r['occluded_response']
                                if r['injected_response']:
                                    frame_info['injected_response'] = r['injected_response']
                                stats_to_save['frames'].append(frame_info)
                            
                            json.dump(stats_to_save, f, indent=2, ensure_ascii=False)
                        
                        scene_stats['occlusion_test'] = {
                            k: v for k, v in stats.items() if k != 'detailed_results'
                        }
                
                all_stats[scene_name] = scene_stats
                print(f"âœ… {scene_name} å®Œæˆ")
                
            except Exception as e:
                print(f"âŒ {scene_name} å¤±æ•—: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # ç”Ÿæˆç¸½çµ
        self._generate_summary(output_dir, scene_dirs, all_stats,
                              occlusion_start, occlusion_end, occlusion_ratio, occlusion_type, occlusion_frames)
        
        print(f"\nğŸ‰ æ‰€æœ‰è¦–è¦ºåŒ–å®Œæˆï¼è¼¸å‡ºç›®éŒ„: {output_dir}")
    
    def _generate_summary(self, output_dir, scene_dirs, all_stats,
                          occlusion_start, occlusion_end, occlusion_ratio, occlusion_type, occlusion_frames=None):
        """ç”Ÿæˆè¦–è¦ºåŒ–ç¸½çµ"""
        
        summary = {
            'timestamp': datetime.now().isoformat(),
            'checkpoint': str(self.checkpoint_path),  # è¨˜éŒ„ä½¿ç”¨çš„ checkpoint
            'total_scenes': len(scene_dirs),
            'scenes': [d.name for d in scene_dirs],
            'occlusion_config': {
                'start': occlusion_start,
                'end': occlusion_end,
                'frames': occlusion_frames,
                'ratio': occlusion_ratio,
                'type': occlusion_type
            },
            'outputs_per_scene': [
                'temporal_consistency.mp4',
                'depth_ordering.mp4',
                'trajectory.mp4',
                'occlusion_test.mp4',
                'occlusion_results.json'
            ],
            'stats': all_stats
        }
        
        with open(output_dir / 'summary.json', 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆ README
        readme = f"""# TempoVLM Complete Demo Results

## ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ç¸½å…± {len(scene_dirs)} å€‹å ´æ™¯

| å ´æ™¯ | æ™‚åºä¸€è‡´æ€§ | æ·±åº¦æ’åº | è»Œè·¡é æ¸¬ | é®æ“‹æ¸¬è©¦ |
|------|-----------|---------|---------|---------|
"""
        for scene_dir in scene_dirs:
            scene_name = scene_dir.name
            readme += f"| {scene_name} | âœ… | âœ… | âœ… | âœ… |\n"
        
        readme += f"""
## æ¯å€‹å ´æ™¯åŒ…å«:

1. **temporal_consistency.mp4** - æ™‚åºä¸€è‡´æ€§å°æ¯”å½±ç‰‡
   - Base Model vs Unified Model ç‰¹å¾µç›¸ä¼¼åº¦æ›²ç·š

2. **depth_ordering.mp4** - æ·±åº¦æ’åºæ¸¬è©¦
   - ä¸‰å€‹å€åŸŸ (å·¦/ä¸­/å³) æ·±åº¦æ¯”è¼ƒ

3. **trajectory.mp4** - è»Œè·¡é æ¸¬
   - ä¿¯è¦–åœ–é¡¯ç¤º GT è»Œè·¡

4. **occlusion_test.mp4** - é®æ“‹æ¸¬è©¦ â­ NEW
   - é®æ“‹é…ç½®: {f"Frames {occlusion_frames}" if occlusion_frames else f"Frame {occlusion_start}-{occlusion_end}"}, ratio={occlusion_ratio}, type={occlusion_type}
   - GT / é®æ“‹ / æ³¨å…¥å¾Œ æè¿°å°æ¯”

5. **occlusion_results.json** - é®æ“‹æ¸¬è©¦è©³ç´°çµæœ
   - æ¯å¹€çš„ç•°å¸¸åˆ†æ•¸ã€å“è³ªã€æè¿°æ–‡å­—

## çµ±è¨ˆæ‘˜è¦
"""
        
        for scene_name, stats in all_stats.items():
            readme += f"\n### {scene_name}\n"
            
            if 'temporal_consistency' in stats:
                tc = stats['temporal_consistency']
                readme += f"- æ™‚åºä¸€è‡´æ€§æ”¹å–„: {tc.get('improvement', 0):.2f}%\n"
            
            if 'occlusion_test' in stats:
                ot = stats['occlusion_test']
                readme += f"- é®æ“‹æª¢æ¸¬ç‡: {ot.get('detection_rate', 0)*100:.1f}%\n"
                readme += f"- æˆåŠŸæ³¨å…¥æ•¸: {ot.get('successful_injections', 0)}\n"
        
        with open(output_dir / 'README.md', 'w', encoding='utf-8') as f:
            f.write(readme)


def main():
    parser = argparse.ArgumentParser(description='TempoVLM Complete Demo')
    parser.add_argument('--model_path', type=str, required=True,
                       help='UnifiedTempoVLM æ¨¡å‹è·¯å¾‘')
    parser.add_argument('--data_root', type=str, required=True,
                       help='ScanNet è³‡æ–™æ ¹ç›®éŒ„')
    parser.add_argument('--output_dir', type=str, default='./complete_demo_output',
                       help='è¼¸å‡ºç›®éŒ„')
    parser.add_argument('--split', type=str, default='test', choices=['train', 'test', 'all'],
                       help='ä½¿ç”¨å“ªå€‹è³‡æ–™é›†')
    parser.add_argument('--max_scenes', type=int, default=3,
                       help='æœ€å¤šè™•ç†å¹¾å€‹å ´æ™¯')
    parser.add_argument('--device', type=str, default='cuda')
    
    # Demo é¸æ“‡åƒæ•¸
    parser.add_argument('--demos', type=str, default='all',
                       help='è¦åŸ·è¡Œçš„ demoï¼Œç”¨é€—è™Ÿåˆ†éš”: temporal,depth,motion,occlusion æˆ– all')
    
    # é®æ“‹æ¸¬è©¦åƒæ•¸
    parser.add_argument('--occlusion_start', type=int, default=5,
                       help='é–‹å§‹é®æ“‹çš„å¹€æ•¸ï¼Œé è¨­ç¬¬ 5 å¹€')
    parser.add_argument('--occlusion_gap', type=int, default=5,
                       help='é®æ“‹å€é–“é–“éš”ï¼ˆå¹€æ•¸ï¼‰ï¼Œé è¨­ 5 å¹€')
    parser.add_argument('--occlusion_frames', type=str, default=None,
                       help='æŒ‡å®šé®æ“‹å¹€ (ç”¨é€—è™Ÿåˆ†éš”ï¼Œä¾‹å¦‚ "5,8,12")')
    parser.add_argument('--occlusion_ratio', type=float, default=0.4,
                       help='é®æ“‹å€åŸŸæ¯”ä¾‹ï¼ˆç”¨æ–¼ YOLO å¤±æ•—æ™‚çš„å‚™ç”¨é®æ“‹ï¼‰')
    parser.add_argument('--occlusion_type', type=str, default='black',
                       choices=['black', 'white', 'blur', 'noise', 
                               'yolo_indoor', 'yolo_furniture', 'yolo_chair', 'yolo_all'],
                       help='é®æ“‹é¡å‹ (yolo_* éœ€è¦å®‰è£ ultralytics)')
    parser.add_argument('--injection_method', type=str, default='full',
                       choices=['raw', 'full', 'strong', 'adaptive', 'none'],
                       help='æ³¨å…¥æ–¹æ³• (none=ä¸æ³¨å…¥ï¼Œç”¨æ–¼å°æ¯”å¯¦é©—)')
    parser.add_argument('--anomaly_threshold', type=float, default=0.25,
                       help='ç•°å¸¸æª¢æ¸¬é–¾å€¼ (è¶Šä½è¶Šæ•æ„Ÿ)')
    parser.add_argument('--segment_length', type=int, default=3,
                       help='æ¯å€‹é®æ“‹å€é–“çš„é•·åº¦ï¼ˆå¹€æ•¸ï¼‰ï¼Œé è¨­ 3 å¹€')
    
    args = parser.parse_args()
    
    # è§£æ demos åƒæ•¸
    if args.demos == 'all':
        demos = ['temporal', 'depth', 'motion', 'occlusion']
    else:
        demos = [d.strip() for d in args.demos.split(',')]
    
    visualizer = CompleteDemoVisualizer(
        unified_model_path=args.model_path,
        device=args.device
    )
    
    visualizer.run_complete_demo(
        data_root=args.data_root,
        output_dir=args.output_dir,
        split=args.split,
        max_scenes=args.max_scenes,
        occlusion_start=args.occlusion_start,
        occlusion_gap=args.occlusion_gap,
        occlusion_frames=args.occlusion_frames,
        occlusion_ratio=args.occlusion_ratio,
        occlusion_type=args.occlusion_type,
        injection_method=args.injection_method,
        anomaly_threshold=args.anomaly_threshold,
        segment_length=args.segment_length,
        demos=demos
    )


if __name__ == '__main__':
    main()
